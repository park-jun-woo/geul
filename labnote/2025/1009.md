# **[GEUL] 연구 일지: 2025-10-09**

**Project Stream:** `GEUL` / `HPA` / `SSHG`
**Author:** 박준우(mail@parkjunwoo.com)

---

## **용어**

1.  **GEUL (General Embedding Unified Language)**: 범용 임베딩 통합 언어
2.  **HPA (Hypothesis-Driven Parsing Algorithm)**: 가설 기반 구문 분석 알고-리즘
3.  **SSHG (Semantic Superposition Heterogeneous Graph)**: 의미 중첩 이종 그래프

---

## **☀️ 아침: 오늘의 목표 (Objectives)**

1.  **HPA Pruning 파이프라인 실증**: MRS 파싱 결과물에서 추출한 명사 및 동사 노드의 의미 후보(Wikidata Q-ID, WordNet `synset_id`)를 `prune_word_prompt.txt` 템플릿과 `ollama gpt-oss:20b` 모델을 이용하여 문맥에 맞게 Pruning하는 `mrs.py` 스크립트를 완성하고, 고전적인 중의성 문장으로 성능을 테스트한다.
2.  **Factorize 데이터 정제**: 기존 `factorize` 데이터 분석 시 Gemini가 생성했던 환각 `synset_id` 오류들을 교정하는 `validate.py` 워크플로우를 최종 점검한다. 특히, `gpt-oss:20b` 모델이 부정확한 결과를 보이는 문제를 해결하기 위해, 더 강력한 모델(예: Gemini API)을 활용하는 방향을 모색하고 그 타당성을 검토한다.

---

## **⏱️ 오후: 진행 과정 및 발견 (Process & Findings)**

### **1. HPA Pruning 스크립트 (`mrs.py`) 완성 및 테스트**

`mrs.py`에 `LLMPruner` 클래스를 구현하여, ACE 파싱, 지식 연동, LLM Pruning, 최종 필터링으로 이어지는 전체 파이프라인을 완성했다.

* **Case 1: `I saw the man with the telescope.`**
    * 초기 5개의 MRS 파싱 결과에 대해, `nodes` 섹션을 생성하고 각 노드(`see`, `man`, `telescope`, `saw`)의 `knowledge_links`를 성공적으로 연결했다.
    * LLM Pruning을 통해 `see`의 22개 후보는 2개로, `man`의 11개 후보는 2개로 압축되는 등, 각 단어의 의미 후보군이 문맥에 맞게 효과적으로 정제됨을 확인했다.
    * 특히, **'의미 연결 실패에 기반한 가지치기'** 규칙을 구현하여, `lemma: saw`('톱질하다')에 대한 `knowledge_links`가 모두 제거되자, 이 노드를 사용하는 `parse_id: 3, 4`가 **최종 결과에서 성공적으로 Pruning** 되었다. 그 결과, 총 3개의 유효한 해석만 남았다.

* **Case 2: `Time flies like an arrow.`**
    * 초기 19개의 유효한 MRS 파싱 결과를 대상으로 Pruning을 수행했다.
    * LLM은 문맥상 'Time'이 동사('시간을 재다')이거나 'like'가 동사('좋아하다')인 해석이 비상식적이라고 판단, 해당 `knowledge_links`를 모두 제거했다.
    * 그 결과, '의미 연결 실패' 규칙에 따라 해당 해석 그룹 전체가 제거되고, **"시간이 화살처럼 날아간다"**는 핵심 의미를 공유하는 **6개의 파싱 결과만 살아남았다.**
    * **결론**: HPA가 고도의 중의성을 가진 문장에 대해서도 **[가능성 탐색 → 상식적 필터링 → 핵심 중의성 보존]** 이라는 의도한 설계대로 완벽하게 작동함을 실증했다.

### **2. Factorize 데이터 정제: `gpt-oss:20b` vs. `Gemini`**

Gemini가 생성한 상세 의미 분석 자료(JSON)의 환각 오류(`invalid.sememes.json` 등)를 교정하는 `validate.py` 워크플로우에서, LLM을 `gpt-oss:20b`에서 `gemini-2.5-flash`로 변경하여 성능을 비교했다.

* **`gpt-oss:20b`의 한계**:
    * **문맥과 무관한 창작**: `abacinate` 예시에서, 부적절한 후보군이 주어지자 '야구'라는 엉뚱한 문맥을 가정하고 오답을 창작했다.
    * **일관성 없는 추론**: `abstain` 예시에서, 동일한 `synset`에 대해 한 번은 너무 일반적인 답(`action.n.01`)을, 다른 한 번은 너무 구체적인 답(`food.n.01`)을 내놓아 일관성이 부족했다.
    * **새로운 환각 생성**: `abet` 예시에서, `ethical_valence.n.01`을 `ethical_value.n.01`이라는 또 다른 환각 `synset`으로 교정하는 실수를 보였다.

* **`Gemini` 모델의 성능 향상**:
    * **정확한 후보 선택**: `abacinate` 예시에서, 개선된 후보군이 주어지자 문맥에 가장 적합한 `sheet_metal.n.01`을 정확하게 선택했다.
    * **유효한 대안 제안**: `abscond` 예시처럼 후보군이 없는 경우, `comitative.n.01`을 `companion.n.01`이라는 유효하고 적절한 `synset`으로 스스로 제안했다.
    * **정직한 판단**: `abhor` 예시에서, 'experiencer'에 완벽히 부합하는 `synset`이 없다고 판단하고 무리하게 추측하는 대신 `NO_CANDIDATE`를 반환하는 등, 프롬프트 규칙을 더 잘 준수했다.

* **결론**: LLM을 이용한 반자동 데이터 정제는 충분히 가능하지만, `gpt-oss:20b`와 같은 소형 모델은 복잡한 의미 추론에 한계를 보인다. **정교한 프롬프트 엔지니어링**과 **더 강력한 외부 모델(Gemini 등)의 활용**이 고품질 데이터셋 구축의 핵심임을 확인했다.

---

## **🌙 저녁: 결론 및 향후 계획 (Conclusion & Next Steps)**

* **결론 1**: `mrs.py`로 대표되는 HPA의 1차 파이프라인(단일 문장 분석 및 Pruning)은 기술적으로 완성되었으며, 의도한 성능을 성공적으로 보여주었다.
* **결론 2**: 순수 규칙 기반 Pruning은 명백한 오류를 제거하는 데 효과적이지만, 미묘한 문맥적 타당성을 판단하기 위해서는 LLM의 도움이 필수적이다.
* **향후 계획**:
    1.  안정화된 `mrs.py`를 활용하여, 단일 문장이 아닌 **문단 단위(약 2000토큰)의 텍스트**에 대한 분석으로 확장한다.
    2.  `cc_news` 본문 데이터를 사용하여, **상호참조 해결(Coreference Resolution)** 및 **어휘 일관성(Lexical Cohesion)** 등 문단 단위에서만 가능한 새로운 Pruning 규칙들을 구상하고 HPA에 구현할 계획이다.