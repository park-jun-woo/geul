#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import os
import json
import time
import argparse
import psycopg2
from psycopg2.extras import execute_values
from tqdm import tqdm
from typing import List, Dict, Any

# PostgreSQL ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': 'localhost',
    'database': 'geuldev',
    'user': 'postgres',
    'password': 'test1224!'
}

class FactorizedDataToPostgres:
    """
    JSON í˜•ì‹ì˜ ì˜ë¯¸ ë¶„í•´ ë°ì´í„°ë¥¼ PostgreSQLì— ì‚½ì…í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, db_config: Dict[str, str]):
        """PostgreSQL ì—°ê²° ì´ˆê¸°í™”"""
        self.conn = psycopg2.connect(**db_config)
        self.stats = {
            'files_processed': 0,
            'qualifiers': 0,
            'sememes': 0,
            'participants': 0
        }

    def clear_tables(self):
        """ê¸°ì¡´ ë°ì´í„° ì‚­ì œ"""
        print("ê¸°ì¡´ factorized ë°ì´í„° ì‚­ì œ ì¤‘...")
        tables = [
            'wordnet_factorized_participants',
            'wordnet_factorized_sememes',
            'wordnet_factorized_qualifiers'
        ]
        with self.conn.cursor() as cur:
            for table in tables:
                cur.execute(f"TRUNCATE TABLE {table} RESTART IDENTITY CASCADE")
        self.conn.commit()
        print("ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")

    def process_directory(self, dir_path: str, batch_size: int):
        """
        ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ DBì— ì‚½ì…í•©ë‹ˆë‹¤.
        """
        print(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹œì‘: {dir_path}")
        try:
            filepaths = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.json')]
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {dir_path}")
            return
            
        qualifiers_batch = []
        sememes_batch_data = [] # (sememe_tuple, [participant_tuple, ...])
        
        with tqdm(total=len(filepaths), desc="JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘") as pbar:
            for path in filepaths:
                try:
                    pbar.write(f"--> Processing: {os.path.basename(path)}")

                    with open(path, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    synset_id = data.get("synset_id")
                    frame_id = data.get("frame_id")

                    if not synset_id or frame_id is None:
                        continue

                    # 1. Qualifiers ë°ì´í„° ì¤€ë¹„
                    for name, props in data.get("qualifiers", {}).items():
                        if props is None:
                            continue # propsê°€ Noneì´ë©´ ì´ qualifierëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
                        qualifier_value = props.get('value')
                        reasoning = props.get('reasoning')

                        has_valid_value = qualifier_value is not None and qualifier_value != {}
                        # reasoningì´ ì¡´ì¬í•˜ê³ , ê³µë°± ë¬¸ìê°€ ì•„ë‹Œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                        has_reasoning = reasoning and reasoning.strip()

                        if has_valid_value or has_reasoning:
                            # ìœ íš¨í•œ valueê°€ ì•„ë‹ ê²½ìš° DBì— NULLë¡œ ì…ë ¥ë˜ë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
                            db_value = qualifier_value if has_valid_value else None
                            
                            qualifiers_batch.append((
                                synset_id, frame_id, name, db_value, reasoning
                            ))

                    # 2. Sememesì™€ Participants ë°ì´í„° ì¤€ë¹„
                    sememes_data = data.get("sememes", [])

                    # sememes ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ê²½ìš° ì²˜ë¦¬
                    if isinstance(sememes_data, dict):
                        # ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¥¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ë³€í™˜í•˜ëŠ” ë¡œì§ ì¶”ê°€
                        # ì´ ë¶€ë¶„ì€ ë°ì´í„°ì˜ ì •í™•í•œ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
                        # ì˜ˆì‹œ: ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ì˜ sememe ê°ì²´ë¡œ ê°„ì£¼
                        sememe_tuple = (
                            synset_id, frame_id,
                            sememes_data.get("VerbType", {}).get('value'),
                            sememes_data.get("VerbProperty", {}).get('value'),
                            "Converted from dict format" # reasoningì€ ì„ì˜ë¡œ ì§€ì •
                        )
                        # ì´ êµ¬ì¡°ì—ì„œëŠ” participantsë¥¼ íŒŒì‹±í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        participants_list = []
                        sememes_batch_data.append((sememe_tuple, participants_list))

                    # ê¸°ì¡´ì˜ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì²˜ë¦¬
                    elif isinstance(sememes_data, list):
                        for sememe in data.get("sememes", []):
                            sememe_tuple = (
                                synset_id, frame_id,
                                sememe.get('verb_type'), sememe.get('verb_property'), sememe.get('reasoning')
                            )
                            participants_list = []
                            for participant in sememe.get("participants", []):
                                participants_list.append((
                                    participant.get('semantic_role'),
                                    participant.get('value_type'),
                                    participant.get('reasoning')
                                ))
                            sememes_batch_data.append((sememe_tuple, participants_list))

                except (json.JSONDecodeError, KeyError) as e:
                    pbar.write(f"ê²½ê³ : íŒŒì¼ {os.path.basename(path)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
                finally:
                    pbar.update(1)

                # qualifiers ë˜ëŠ” sememes ë°ì´í„° ì¤‘ í•˜ë‚˜ë¼ë„ ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ë„ë‹¬í•˜ë©´ DBì— ì‚½ì…
                if len(qualifiers_batch) >= batch_size or len(sememes_batch_data) >= batch_size:
                    try:
                        self._insert_data_batch(qualifiers_batch, sememes_batch_data)
                        qualifiers_batch, sememes_batch_data = [], []
                    except psycopg2.Error as e: # Exceptionì„ psycopg2.Errorë¡œ ë” êµ¬ì²´í™”
                        pbar.write("\n" + "="*80)
                        pbar.write(f"ğŸ”¥ ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì… ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ!")
                        pbar.write(f"ğŸ”¥ ì—ëŸ¬ ìœ í˜•: {type(e).__name__}")
                        pbar.write(f"ğŸ”¥ ì—ëŸ¬ ë©”ì‹œì§€: {e}")
                        pbar.write("="*80)
                        raise # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ except ë¸”ë¡ì´ ì²˜ë¦¬í•˜ë„ë¡ í•¨

        # ë£¨í”„ ì¢…ë£Œ í›„ ë‚¨ì€ ë°ì´í„° ì‚½ì…
        if qualifiers_batch or sememes_batch_data:
            try:
                self._insert_data_batch(qualifiers_batch, sememes_batch_data)
            except psycopg2.Error as e:
                # â–¼â–¼â–¼â–¼â–¼ [ìˆ˜ì •] ìƒì„¸ ì—ëŸ¬ ë¡œê¹… ì ìš© â–¼â–¼â–¼â–¼â–¼
                print("\n" + "="*80)
                print(f"ğŸ”¥ ë§ˆì§€ë§‰ ë°°ì¹˜ ì‚½ì… ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ!")
                print(f"ğŸ”¥ ì—ëŸ¬ ìœ í˜•: {type(e).__name__}")
                print(f"ğŸ”¥ ì—ëŸ¬ ë©”ì‹œì§€: {e}")
                print("="*80)
                raise

        self.conn.commit()
        print("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ë° DB ì‚½ì… ì™„ë£Œ.")

    def _insert_data_batch(self, qualifiers_batch: List, sememes_batch_data: List):
        """
        ì¤€ë¹„ëœ ë°ì´í„° ë°°ì¹˜ë¥¼ ìœ íš¨ì„± ê²€ì‚¬ í›„ DBì— ì‚½ì…í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
        ë°ì´í„° íƒ€ì… ì˜¤ë¥˜ë¥¼ ì‚¬ì „ì— ê²€ì‚¬í•˜ì—¬ ìƒì„¸í•œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        """
        with self.conn.cursor() as cur:
            try:
                # 1. Qualifiers ì²˜ë¦¬
                if qualifiers_batch:
                    # DBì— ë³´ë‚´ê¸° ì „ ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬
                    for q_row in qualifiers_batch:
                        # q_row = (synset_id, frame_id, name, value, reasoning)
                        for item in q_row:
                            if isinstance(item, (dict, list)):
                                raise TypeError(
                                    f"Qualifiers ë°ì´í„° íƒ€ì… ì˜¤ë¥˜. synset_id='{q_row[0]}', frame_id={q_row[1]}. "
                                    f"ë¬¸ì œ í•„ë“œ '{q_row[2]}', ë¬¸ì œ ê°’: {json.dumps(item, ensure_ascii=False)}"
                                )
                    # Qualifiers ì‚½ì…
                    execute_values(cur, """
                        INSERT INTO wordnet_factorized_qualifiers (synset_id, frame_id, qualifier_name, value, reasoning)
                        VALUES %s ON CONFLICT DO NOTHING
                    """, qualifiers_batch)
                    self.stats['qualifiers'] += len(qualifiers_batch)

                # 2. Sememesì™€ Participants ì²˜ë¦¬
                if sememes_batch_data:
                    sememes_to_insert = [s_data[0] for s_data in sememes_batch_data]

                    # Sememes ìœ íš¨ì„± ê²€ì‚¬
                    for s_row in sememes_to_insert:
                        # s_row = (synset_id, frame_id, verb_type, verb_property, reasoning)
                        for item in s_row:
                            if isinstance(item, (dict, list)):
                                raise TypeError(
                                    f"Sememes ë°ì´í„° íƒ€ì… ì˜¤ë¥˜. synset_id='{s_row[0]}', frame_id={s_row[1]}. "
                                    f"ë¬¸ì œ ê°’: {json.dumps(item, ensure_ascii=False)}"
                                )
                    
                    # Sememes ì‚½ì… ë° ìƒì„±ëœ ID ë°˜í™˜
                    inserted_sememe_ids = execute_values(cur, """
                        INSERT INTO wordnet_factorized_sememes (synset_id, frame_id, verb_type, verb_property, reasoning)
                        VALUES %s RETURNING sememe_id
                    """, sememes_to_insert, fetch=True)
                    self.stats['sememes'] += len(inserted_sememe_ids)

                    # Participants ë°ì´í„° ì¤€ë¹„ ë° ìœ íš¨ì„± ê²€ì‚¬
                    participants_to_insert = []
                    for i, sememe_id_tuple in enumerate(inserted_sememe_ids):
                        sememe_id = sememe_id_tuple[0]
                        participants_list = sememes_batch_data[i][1]
                        parent_sememe = sememes_to_insert[i] # ì—ëŸ¬ ë¡œê¹…ìš©

                        for p_tuple in participants_list:
                            # p_tuple = (semantic_role, value_type, reasoning)
                            for item in p_tuple:
                                if isinstance(item, (dict, list)):
                                    raise TypeError(
                                        f"Participants ë°ì´í„° íƒ€ì… ì˜¤ë¥˜. synset_id='{parent_sememe[0]}', frame_id={parent_sememe[1]}. "
                                        f"ë¬¸ì œ ê°’: {json.dumps(item, ensure_ascii=False)}"
                                    )
                            participants_to_insert.append((sememe_id,) + p_tuple)
                    
                    # Participants ì‚½ì…
                    if participants_to_insert:
                        execute_values(cur, """
                            INSERT INTO wordnet_factorized_participants (sememe_id, semantic_role, value_type, reasoning)
                            VALUES %s
                        """, participants_to_insert)
                        self.stats['participants'] += len(participants_to_insert)

            except (Exception, psycopg2.Error) as e:
                # ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ DB ìƒíƒœë¥¼ ë˜ëŒë¦¬ê³ ,
                # ìƒìœ„ ì—ëŸ¬ í•¸ë“¤ëŸ¬ê°€ ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥í•˜ë„ë¡ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ì „ë‹¬
                self.conn.rollback()
                raise e

    def print_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n========== ì²˜ë¦¬ ì™„ë£Œ ==========")
        print(f"Qualifiers ì‚½ì…: {self.stats['qualifiers']:,}ê°œ")
        print(f"Sememes ì‚½ì…: {self.stats['sememes']:,}ê°œ")
        print(f"Participants ì‚½ì…: {self.stats['participants']:,}ê°œ")
        print("==============================")
        
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.conn.close()
        print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ.")


def main():
    parser = argparse.ArgumentParser(description="Factorized WordNet JSON ë°ì´í„°ë¥¼ PostgreSQLì— ì‚½ì…í•©ë‹ˆë‹¤.")
    parser.add_argument("--input-dir", type=str, default="geulso/factorize/factorized/", help="ì…ë ¥ JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--batch-size", type=int, default=1000, help="í•œ ë²ˆì— DBì— ì‚½ì…í•  ë ˆì½”ë“œ ìˆ˜")
    parser.add_argument("--clear", action='store_true', help="ê¸°ì¡´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì‚½ì…í•©ë‹ˆë‹¤.")
    args = parser.parse_args()

    loader = FactorizedDataToPostgres(DB_CONFIG)
    
    try:
        start_time = time.time()
        
        if args.clear:
            loader.clear_tables()
        
        loader.process_directory(args.input_dir, args.batch_size)
        loader.print_stats()
        
        elapsed = time.time() - start_time
        print(f"\nì „ì²´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {e}")
        loader.conn.rollback()
        
    finally:
        loader.close()

if __name__ == "__main__":
    main()