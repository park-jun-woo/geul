#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import nltk
from nltk.corpus import wordnet as wn
import psycopg2
from psycopg2.extras import execute_batch
import time
from datetime import datetime
from typing import Dict, Set
from tqdm import tqdm

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
try:
    nltk.data.find('corpora/wordnet.zip')
except nltk.downloader.DownloadError:
    nltk.download('wordnet')

try:
    nltk.data.find('corpora/omw-1.4.zip')
except nltk.downloader.DownloadError:
    nltk.download('omw-1.4')

class WordNetToPostgres:
    def __init__(self, db_config: Dict[str, str]):
        self.conn = psycopg2.connect(**db_config)
        self.cursor = self.conn.cursor()
        self.stats = {
            'synsets': 0, 'lemmas': 0, 'relations': 0,
            'verb_frames': 0, 'multilingual': 0
        }
        self.batch_size = 1000
        self.existing_synset_ids = self._load_existing_synset_ids()

    def _load_existing_synset_ids(self) -> Set[str]:
        print("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ Synset ID ë¡œë“œ ì¤‘...")
        with self.conn.cursor() as cur:
            cur.execute("SELECT synset_id FROM wordnet_synsets")
            synset_ids = {row[0] for row in cur.fetchall()}
        print(f"-> {len(synset_ids):,}ê°œì˜ ê¸°ì¡´ Synset ID ë¡œë“œ ì™„ë£Œ.")
        return synset_ids

    def clear_tables(self):
        print("ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        tables = [
            'wordnet_lemma_relations', 'wordnet_synset_relations',
            'wordnet_verb_frames', 'wordnet_multilingual',
            'wordnet_wikidata_mapping', 'wordnet_lemmas',
            'wordnet_synsets', 'wordnet_metadata'
        ]
        
        for table in tables:
            self.cursor.execute(f"TRUNCATE TABLE {table} CASCADE")
        self.conn.commit()
        print("ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
    def insert_metadata(self):
        self.cursor.execute("TRUNCATE TABLE wordnet_metadata")
        self.cursor.execute("""
            INSERT INTO wordnet_metadata (version, language, imported_at)
            VALUES (%s, %s, %s)
        """, ('3.0', 'en', datetime.now()))
        self.conn.commit()
        
    def process_synsets(self):
        print("Synsets ì²˜ë¦¬ ì¤‘ (ì‹ ê·œ ë°ì´í„°ë§Œ ì„ ë³„)...")
        synsets_batch = []
        lemmas_batch = []
        
        all_synsets = list(wn.all_synsets())
        for synset in tqdm(all_synsets, desc="Synsets/Lemmas ì²˜ë¦¬"):
            try: 
                synset_id = synset.name()
                if synset_id in self.existing_synset_ids:
                    continue
                
                pos = synset.pos()
                lexname = synset.lexname()
                definition = synset.definition()
                examples = '; '.join(synset.examples()) if synset.examples() else None
                
                synsets_batch.append((
                    synset_id, pos, lexname, definition, examples,
                    definition + (' ' + examples if examples else '')
                ))
                
                for lemma in synset.lemmas():
                    lemmas_batch.append((
                        synset_id, lemma.name().replace('_', ' '),
                        lemma.key(), synset.offset(), lemma.count()
                    ))
                
                if len(synsets_batch) >= self.batch_size:
                    self._insert_synsets_batch(synsets_batch)
                    synsets_batch = []
                if len(lemmas_batch) >= self.batch_size:
                    self._insert_lemmas_batch(lemmas_batch)
                    lemmas_batch = []
            except Exception as e:
                tqdm.write(f"ê²½ê³ : Synset {synset.name()} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                continue
                
        if synsets_batch: self._insert_synsets_batch(synsets_batch)
        if lemmas_batch: self._insert_lemmas_batch(lemmas_batch)
        print(f"ì‹ ê·œ Synsets: {self.stats['synsets']}ê°œ, ì‹ ê·œ Lemmas: {self.stats['lemmas']}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
    def _insert_synsets_batch(self, batch):
        execute_batch(self.cursor, """
            INSERT INTO wordnet_synsets (synset_id, pos, lexname, definition, example, gloss)
            VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (synset_id) DO NOTHING
        """, batch)
        self.stats['synsets'] += len(batch)
        self.conn.commit()
        
    def _insert_lemmas_batch(self, batch):
        execute_batch(self.cursor, """
            INSERT INTO wordnet_lemmas (synset_id, word, lemma_key, sense_number, tag_count)
            VALUES (%s, %s, %s, %s, %s)
        """, batch)
        self.stats['lemmas'] += len(batch)
        self.conn.commit()

    def process_relations(self):
        print("Synset ê´€ê³„ ì²˜ë¦¬ ì¤‘ (ON CONFLICTë¡œ ì¤‘ë³µ ë°©ì§€)...")
        relations_batch = []
        relation_types = [
            ('hypernym', lambda s: s.hypernyms()), ('hyponym', lambda s: s.hyponyms()),
            ('instance_hypernym', lambda s: s.instance_hypernyms()), ('instance_hyponym', lambda s: s.instance_hyponyms()),
            ('member_holonym', lambda s: s.member_holonyms()), ('part_holonym', lambda s: s.part_holonyms()), ('substance_holonym', lambda s: s.substance_holonyms()),
            ('member_meronym', lambda s: s.member_meronyms()), ('part_meronym', lambda s: s.part_meronyms()), ('substance_meronym', lambda s: s.substance_meronyms()),
            ('similar_to', lambda s: s.similar_tos()), ('attribute', lambda s: s.attributes()),
            ('entailment', lambda s: s.entailments()), ('cause', lambda s: s.causes()),
            ('also_see', lambda s: s.also_sees()), ('verb_group', lambda s: s.verb_groups()),
            ('topic_domain', lambda s: s.topic_domains()), ('region_domain', lambda s: s.region_domains()), ('usage_domain', lambda s: s.usage_domains())
        ]
        
        for synset in tqdm(wn.all_synsets(), desc="ê´€ê³„ ì²˜ë¦¬"):
            synset_id = synset.name()
            for rel_type, rel_func in relation_types:
                try: 
                    related = rel_func(synset)
                    for related_synset in related:
                        relations_batch.append((synset_id, related_synset.name(), rel_type))
                        if len(relations_batch) >= self.batch_size:
                            self._insert_relations_batch(relations_batch)
                            relations_batch = []
                except Exception as e:
                    tqdm.write(f"ê²½ê³ : ê´€ê³„ '{rel_type}' ì²˜ë¦¬ ì¤‘ {synset_id}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                    continue
                    
        if relations_batch: self._insert_relations_batch(relations_batch)
        print(f"ê´€ê³„: {self.stats['relations']}ê°œ ì‚½ì… ì‹œë„ ì™„ë£Œ")
        
    def _insert_relations_batch(self, batch):
        execute_batch(self.cursor, """
            INSERT INTO wordnet_synset_relations (from_synset, to_synset, relation_type)
            VALUES (%s, %s, %s) ON CONFLICT (from_synset, to_synset, relation_type) DO NOTHING
        """, batch)
        self.stats['relations'] += len(batch)
        self.conn.commit()
        
    def process_verb_frames(self):
        print("ë™ì‚¬ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ (ON CONFLICTë¡œ ì¤‘ë³µ ë°©ì§€)...")
        frames_batch = []
        for synset in tqdm(wn.all_synsets(pos='v'), desc="ë™ì‚¬ í”„ë ˆì„ ì²˜ë¦¬"):
            try: 
                synset_id = synset.name()
                unique_frames = set()
                for lemma in synset.lemmas():
                    unique_frames.update(lemma.frame_strings())
                
                for frame_id, frame_text in enumerate(sorted(list(unique_frames))):
                    frames_batch.append((synset_id, frame_id, frame_text))
                
                if len(frames_batch) >= self.batch_size:
                    self._insert_verb_frames_batch(frames_batch)
                    frames_batch = []
            except Exception as e:
                tqdm.write(f"ê²½ê³ : ë™ì‚¬ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ {synset.name()}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ, ê±´ë„ˆëœë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                continue
                        
        if frames_batch: self._insert_verb_frames_batch(frames_batch)
        print(f"ë™ì‚¬ í”„ë ˆì„: {self.stats['verb_frames']}ê°œ ì‚½ì… ì‹œë„ ì™„ë£Œ")
        
    def _insert_verb_frames_batch(self, batch):
        execute_batch(self.cursor, """
            INSERT INTO wordnet_verb_frames (synset_id, frame_id, frame_text)
            VALUES (%s, %s, %s) ON CONFLICT (synset_id, frame_id) DO NOTHING
        """, batch)
        self.stats['verb_frames'] += len(batch)
        self.conn.commit()
        
    def process_multilingual(self):
        print("ë‹¤êµ­ì–´ ë°ì´í„° ì²˜ë¦¬ ì¤‘ (ON CONFLICTë¡œ ì¤‘ë³µ ë°©ì§€)...")
        multi_batch = []
        languages = wn.langs()
        
        for lang in tqdm(languages, desc="ë‹¤êµ­ì–´ ì²˜ë¦¬"):
            if lang == 'en': continue
            try:
                for synset in wn.all_synsets():
                    for lemma in synset.lemmas(lang=lang):
                        multi_batch.append((synset.name(), lang, lemma.name().replace('_', ' ')))
                        if len(multi_batch) >= self.batch_size:
                            self._insert_multilingual_batch(multi_batch)
                            multi_batch = []
            except Exception as e:
                tqdm.write(f"ê²½ê³ : ë‹¤êµ­ì–´ '{lang}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, í•´ë‹¹ ì–¸ì–´ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                continue
                
        if multi_batch: self._insert_multilingual_batch(multi_batch)
        print(f"ë‹¤êµ­ì–´: {self.stats['multilingual']}ê°œ ì‚½ì… ì‹œë„ ì™„ë£Œ")
        
    def _insert_multilingual_batch(self, batch):
        execute_batch(self.cursor, """
            INSERT INTO wordnet_multilingual (synset_id, language, word)
            VALUES (%s, %s, %s) ON CONFLICT (synset_id, language, word) DO NOTHING
        """, batch)
        self.stats['multilingual'] += len(batch)
        self.conn.commit()
        
    def print_stats(self):
        print("\n========== ì²˜ë¦¬ ì™„ë£Œ ==========")
        print(f"ì‹ ê·œ Synsets: {self.stats['synsets']:,}ê°œ")
        print(f"ì‹ ê·œ Lemmas: {self.stats['lemmas']:,}ê°œ")
        print(f"ê´€ê³„ (ì‚½ì… ì‹œë„): {self.stats['relations']:,}ê°œ")
        print(f"ë™ì‚¬ í”„ë ˆì„ (ì‚½ì… ì‹œë„): {self.stats['verb_frames']:,}ê°œ")
        print(f"ë‹¤êµ­ì–´ (ì‚½ì… ì‹œë„): {self.stats['multilingual']:,}ê°œ")
        print("==============================")
        
    def close(self):
        self.cursor.close()
        self.conn.close()

def main():
    db_config = {
        'host': 'localhost',
        'database': 'geuldev',
        'user': 'postgres',
        'password': 'test1224!'
    }
    
    loader = WordNetToPostgres(db_config)
    current_process = "" 
    try:
        start_time = time.time()
        
        # ë°ì´í„°ë¥¼ ì™„ì „íˆ ìƒˆë¡œ ë„£ìœ¼ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
        # print("ê²½ê³ : ëª¨ë“  í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. 5ì´ˆ í›„ ì‹œì‘...")
        # time.sleep(5)
        loader.clear_tables()
        loader.existing_synset_ids = set() # clear í›„ì—ëŠ” ë©”ëª¨ë¦¬ì˜ ì§‘í•©ë„ ë¹„ì›Œì¤˜ì•¼ í•¨
        
        current_process = "insert_metadata"
        loader.insert_metadata()
        
        current_process = "process_synsets"
        loader.process_synsets()

        current_process = "process_relations"
        loader.process_relations()

        current_process = "process_verb_frames"
        loader.process_verb_frames()

        current_process = "process_multilingual"
        loader.process_multilingual()
        
        loader.print_stats()
        
        elapsed = time.time() - start_time
        print(f"\nì „ì²´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        
    except Exception as e:
        print("\n" + "="*80)
        print(f"ğŸ”¥ '{current_process}' ì‘ì—… ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ê°€ ë°œìƒí•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ”¥ ì—ëŸ¬ ìœ í˜•: {type(e).__name__}")
        print(f"ğŸ”¥ ì—ëŸ¬ ë©”ì‹œì§€: {e}")
        print("="*80)
        loader.conn.rollback()
        
    finally:
        loader.close()

if __name__ == "__main__":
    main()