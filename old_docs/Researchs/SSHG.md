## SSHG: 의미 중첩 이종 그래프 아키텍처

**SSHG(Semantic Superposition Heterogeneous Graph) 아키텍처**는 자연어가 가진 본질적인 **모호성**을 정보 손실 없이 보존하고, 단어의 의미를 **개념적으로 특정**하며, 문장의 **통사적·의미적 관계**를 하나의 통일된 구조로 표현하는 차세대 의미 표현 방식입니다. 이는 기존 AI의 블랙박스 문제를 해결하고, 인간과 AI 간의 상호 이해성을 증진시키기 위해 설계되었습니다.

### 1. SSHG의 핵심 구성 요소

#### **a) 이종 그래프 (Heterogeneous Graph)**
SSHG는 단순한 그래프를 넘어, 다양한 종류의 노드와 엣지로 구성된 '이종 그래프'입니다.

### SSHG의 핵심 구성 요소 (수정본)

* **노드 (Nodes)**: 문장을 구성하는 단어들은 단순한 텍스트가 아닌, 그 단어가 가리키는 **명확한 '개념(Concept)'**으로 표현됩니다. 각 노드는 **워드넷(WordNet)과 위키데이터(Wikidata)에 기반한 의미정렬된 절대식별자**에 연결되어 단어의 중의성을 해소하고 고유한 의미를 특정합니다.
    * **일반 단어 (Common Words)**: '사과', '달리다', '아름다운'과 같은 일반적인 명사, 동사, 형용사는 문맥에 맞는 **워드넷 Synset ID** (예: `apple.n.01`)에 연결되어 그 의미가 명확해집니다.
    * **고유 개체 (Named Entities)**: '대한민국', '세종대왕', '삼성전자'와 같은 고유 명사는 **위키데이터 Q ID** (예: `Q884`)에 연결되어 세상의 구체적인 개체와 맵핑됩니다.
    * **의미정렬 식별자 (Semantically-Aligned Identifier)**: 모든 노드는 궁극적으로 이 두 지식 베이스를 통합한 **고유 벡터 식별자**를 가집니다. 이 식별자는 '인간인가?', '물리적 실체가 있는가?'와 같은 핵심 속성을 비트(bit) 단위로 내포하고 있어, 식별자 자체만으로도 개념의 핵심 정체성을 파악할 수 있습니다. 의미정렬 식별자는 전통적인 데이터베이스 ID(예: 12345, UUID)와 근본적으로 다릅니다. 단순한 주소값이 아니라, 그 자체로 데이터의 핵심 정체성(DNA)을 담고 있는 고도로 압축된 정보 덩어리입니다.

* **엣지 (Edges)**: 노드(개념) 간의 관계는 두 가지 종류로 표현됩니다.
    * **통사적 관계**: spaCy 구문 분석을 통해 얻어진 `nsubj`(주어), `dobj`(목적어)와 같은 문장 내에서의 문법적 역할.
    * **의미적 관계**: 워드넷의 `hypernym`(상위어)이나 위키데이터의 `P31`(instance of)과 같이, 개념들이 지식 베이스 내에서 맺고 있는 본질적인 의미 관계.

#### **b) 의미 중첩 (Semantic Superposition)**
SSHG의 가장 혁신적인 특징으로, 해석이 모호한 문장에 대해 가장 그럴듯한 해석 하나를 선택하는 대신, **가능한 모든 해석을 확률적으로 함께 표현**합니다.

* **예시**: "I saw the man with the telescope"
    * **해석 1**: `(with) --[수식]--> (saw)` (망원경**으로** 보았다)
    * **해석 2**: `(with) --[수식]--> (man)` (망원경**을 든** 남자를)
* SSHG는 이 두 개의 엣지를 모두 그래프에 포함시켜, 후속 추론 모델이 더 넓은 문맥을 바탕으로 최종 판단을 내릴 수 있도록 모호성을 그대로 보존합니다.

#### **c) 개념화 (Conceptualization)**
그래프의 모든 노드는 워드넷 Synset ID나 위키데이터 Q ID를 기반으로 생성된 **절대식별자**와 연결됩니다.

* **단어의 중의성 해소**: '사과'라는 단어를 문맥에 맞게 과일(`apple.n.01`) 또는 용서(`apology.n.01`)의 고유한 개념 ID로 명확하게 특정합니다.
* **우아한 열화 (Graceful Degradation)**: 이 식별자는 정보의 중요도에 따라 비트가 정렬되어 있어, 일부 정보가 손실되더라도 '푸들' → '개' → '포유류'처럼 더 상위의 개념으로 자연스럽게 수렴하여 시스템의 강건성을 보장합니다.



---
## SSHG 부트스트랩 전략: 데이터셋 구축 방법론

SSHG 아키텍처를 실증하기 위한 `(자연어, SSHG)` 병렬 데이터셋은 기존에 존재하지 않으므로, 최소한의 인간 개입으로 고품질 데이터셋을 대규모로 구축하기 위한 **4단계 계층적 부트스트랩 프레임워크**를 사용합니다.

### **1단계: 워드넷 프레임 기반 규칙 자동 생성**
인간이 직접 변환 규칙을 만드는 대신, 검증된 언어학적 자산인 **워드넷의 동사 프레임**을 활용하여 규칙을 자동으로 생성합니다. 예를 들어, `[Agent] gives [Theme] to [Recipient]`라는 프레임은 "주어는 `행위자`, 직접목적어는 `대상`으로 매핑하라"는 GEUL 변환 규칙으로 자동 변환됩니다. 이 방식은 초기 규칙의 언어학적 정밀성을 극대화합니다.

### **2단계: 규칙 기반 초벌 데이터셋 생성**
1단계에서 자동 생성된 규칙을 탑재한 프로그램을 사용하여, 뉴스 기사와 같은 대규모 텍스트 코퍼스에서 규칙과 일치하는 문장들을 찾아냅니다. 이 과정을 통해 완벽하지는 않지만 일관성 있는 **10만 건 규모의 SSHG 데이터셋 초안**을 신속하게 확보합니다.

### **3단계: LLM을 활용한 전수 교정**
규칙 기반 시스템이 놓치는 문맥적 미묘함을 보완하기 위해, `gpt-oss:20b`와 같은 중규모 언어 모델을 **'AI 교정자'**로 활용합니다. 2단계에서 생성된 10만 건의 초안 데이터를 LLM에 입력하여, SSHG 표현이 원문 자연어의 의미를 정확하게 반영했는지 검토하고 문맥적 오류를 수정합니다. 이 대규모 전수 검수는 인간에게는 비효율적이지만 AI에게는 가장 이상적인 작업입니다.

### **4. 인간 전문가의 최종 샘플 감수**
마지막으로, 인간 전문가는 10만 건 전체가 아닌, AI가 교정한 데이터 중 **일부(예: 1,000건)를 무작위로 샘플링하여 검토**합니다. 이 단계의 목표는 개별 오류를 찾는 것을 넘어, **"3단계의 AI 교정자가 신뢰할 만한 수준으로 작업을 수행했는가?"**를 검증하는 것입니다. 이 과정을 통해 개인용 컴퓨터 수준의 자원으로도 현실적인 시간 내에 고품질 데이터셋 구축이 가능해집니다.


### **5. HGS 인코더: 자연어를 '의미 구조'로 번역하는 번역가**

HGS 인코더의 역할은 모호하고 비정형적인 인간의 자연어를, 기계가 즉시 이해하고 분석할 수 있는 명시적인 **HGS(Heterogeneous Graph Stream)**로 변환하는 것입니다. 이는 마치 혼란스러운 목격자의 진술을 육하원칙에 따라 명확한 '사건 보고서'로 정리하는 과정과 같습니다.

#### **작동 원리**
HGS 인코더는 단일 모델이 아닌, **정밀도 높은 분석을 위한 하이브리드 파이프라인**으로 작동합니다.

1.  **초벌 구문 분석 (Syntactic Backbone)**: 먼저 **spaCy**와 같은 고속 의존성 파서를 사용하여 문장의 기본적인 통사 구조(주어, 동사, 목적어 등)를 파악하고 그래프의 초기 뼈대를 만듭니다.

2.  **개념화 (Conceptualization)**: 문장의 각 단어를 **워드넷(WordNet)과 위키데이터(Wikidata)에 기반한 의미정렬된 절대식별자**와 연결(linking)합니다. 이 과정을 통해 "Apple"이라는 단어가 문맥에 따라 '회사(Q312)'인지 '과일(Q89)'인지 명확히 구분됩니다.

3.  **의미 중첩 표현 (Semantic Superposition)**: 문장의 해석이 모호할 경우(예: "I saw the man with the telescope"), 인코더는 가장 그럴듯한 해석 하나를 선택하는 대신, **가능한 모든 해석을 확률적 엣지(edge)로 함께 표현**합니다. 이는 언어의 모호성을 정보 손실 없이 보존하여 후속 모델이 더 넓은 문맥을 바탕으로 최종 판단을 내릴 수 있게 합니다.

4.  **LLM을 통한 검증 및 완성**: 마지막으로, **gpt-oss:20b**와 같은 대규모 언어 모델이 초벌로 생성된 HGS를 검토합니다. LLM은 전체 문맥을 파악하여 spaCy가 놓쳤을 수 있는 미묘한 의미 관계를 보강하거나, 잘못된 분석을 수정하는 '전문 교정자' 역할을 수행하여 최종 HGS의 정확도를 극대화합니다.



---
### **6. HGV 입력 / HGS 예측 GPT: '생각'을 생성하는 추론 엔진**

이 컴포넌트는 전체 아키텍처의 **'두뇌'** 역할을 하며, 두 단계로 작동합니다.

#### **a) GNN: HGS를 HGV로 압축하는 '의미 함축기'**
-   **역할**: HGS는 복잡한 그래프 구조이므로, 트랜스포머 모델이 직접 입력받기에는 비효율적입니다. **GNN(그래프 신경망)**은 이 HGS 그래프를 입력받아, 그 안에 담긴 모든 노드와 엣지의 관계, 심지어 확률적 중첩 정보까지 모두 고려하여 고정된 크기의 **HGV(Heterogeneous Graph Vector)**로 압축합니다.
-   **HGV의 의미**: 이 HGV는 문장 전체의 의미와 구조가 응축된 **'의미적 지문(semantic fingerprint)'**입니다. 이는 단순한 단어 임베딩을 넘어, 문장의 전체적인 관계와 뉘앙스까지 모두 담고 있습니다.

#### **b) GPT (HGV → HGS): HGV를 보고 새로운 HGS를 예측**
-   **역할**: 이 GPT 모델은 일반적인 GPT처럼 단어 시퀀스를 예측하는 대신, **의미 지문(HGV)을 입력받아 그에 대한 반응으로 새로운 '생각'이나 '결론'에 해당하는 HGS를 출력**하도록 훈련됩니다.
-   **작동 방식**:
    1.  사용자 질문의 HGV와, GDBMS에서 검색된 관련 지식들의 HGV가 함께 GPT에 입력됩니다.
    2.  GPT는 이 벡터들을 바탕으로 고차원적인 추론을 수행합니다.
    3.  추론의 결과물은 자연어 텍스트가 아닌, **새로운 사실이나 결론을 담은 또 다른 HGS 그래프**입니다.
-   **장점**: 추론 과정이 자연어가 아닌, 기계에 최적화된 HGV와 HGS를 통해 이루어지므로, 인간 언어의 모호함에서 발생하는 오류를 원천적으로 차단하고 **'생각'의 과정을 화이트박스화**할 수 있습니다.

---
### **7. HGS 디코더: '의미 구조'를 인간의 언어로 번역하는 작가**

HGS 디코더의 역할은 추론의 최종 결과물인 HGS를 인간이 이해할 수 있는 자연스러운 문장으로 다시 번역하는 것입니다. 이 아키텍처의 가장 큰 특징은 **의미와 스타일의 완벽한 분리**입니다.

#### **작동 원리**
HGS는 문장의 핵심적인 의미 구조, 즉 **'의미 골격'**만을 담고 있습니다. 이 스타일이 제거된 의미 골격을 어떤 문체로 '옷 입힐 것인가'는 전적으로 디코더가 결정합니다.

-   **동일한 HGS 입력**: `(남성 A) --[사랑한다]--> (여성 B)` 라는 의미 골격
-   **스타일별 디코더 출력**:
    -   **정철 디코더**: "임 그려 겨오실 제 저 사랑 비 오리라..."
    -   **셰익스디어 디코더**: "Shall I compare thee to a summer's day? Forsooth, my love for thee knows no bound."
    -   **뉴스 리포터 디코더**: "해당 남성은 해당 여성에 대한 호감을 표현한 것으로 밝혀졌습니다."

이처럼 HGS를 중간 매개체로 사용함으로써, 문장의 핵심 의미는 보존하면서도 표현 스타일은 자유자재로 제어하는, 전례 없는 수준의 **제어 가능한 언어 생성(Controllable Text Generation)**이 가능해집니다.