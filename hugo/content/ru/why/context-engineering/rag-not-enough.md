---
title: "Почему RAG недостаточно"
weight: 2
date: 2026-02-26T12:00:11+09:00
lastmod: 2026-02-26T12:00:11+09:00
tags: ["RAG", "поиск", "эмбеддинг"]
summary: "Выглядеть релевантным и быть релевантным -- не одно и то же"
author: "Джуну Пак"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## Выглядеть релевантным и быть релевантным -- не одно и то же.

---

## RAG -- текущий стандарт

По состоянию на 2024 год RAG -- самый распространённый способ, которым предприятия используют LLM.

Retrieval-Augmented Generation.
Поиск по внешним документам, вставка их в контекст, и генерация ответа моделью на основе найденного.

RAG работает.
Он позволяет LLM обращаться к внутренним документам, на которых модель никогда не обучалась.
Он позволяет отражать актуальную информацию.
Он значительно снижает галлюцинации.

Без RAG корпоративное внедрение LLM шло бы куда медленнее.
RAG -- технология, заслуживающая уважения.

Но у RAG есть фундаментальные ограничения.
Эти ограничения не решаются построением лучшего RAG.
Они вытекают из самой предпосылки RAG.

---

## Как работает RAG

Суть RAG -- три шага.

**Шаг 1: Разбить документы на чанки.**
PDF, вики, внутренние документы делятся на фрагменты фиксированного размера (обычно 200--500 токенов).

**Шаг 2: Преобразовать каждый чанк в эмбеддинг-вектор.**
Вещественный вектор из сотен или тысяч измерений.
«Смысл» текста, отображённый в одну точку в векторном пространстве.

**Шаг 3: При поступлении запроса найти похожие векторы.**
Запрос тоже преобразуется в вектор.
Отбираются 5--20 чанков с наибольшим cosine similarity и вставляются в контекст.

Просто и элегантно.
И здесь кроются три фундаментальные проблемы.

---

## Проблема 1: Похожее -- не значит релевантное

Эмбеддинг-сходство измеряет, «используют ли два текста похожие слова в похожих контекстах».

Это не релевантность.

Пример.

Запрос: «Какова выручка Apple за Q3 2024?»

Эмбеддинг-поиск может вернуть:
- «Выручка Apple за Q3 2024 составила $94,9 млрд.» -- Релевантно
- «Выручка Apple за Q3 2023 составила $81,8 млрд.» -- Похоже, но другой период
- «Выручка Samsung Electronics за Q3 2024 составила 79 трлн вон.» -- Похоже, но другая компания
- «В яблочном пироге примерно 296 ккал.» -- Совпадение ключевых слов

Эмбеддинг-сходство не может различить эти четыре случая.
В векторном пространстве «выручка Apple» группируется в одну область.
2023 или 2024, Apple или Samsung --
векторное расстояние не разделяет их надёжно.

Добавление reranker улучшает ситуацию.
Но reranker тоже читает и оценивает текст на естественном языке,
поэтому фундаментальная проблема неоднозначности остаётся.

Поиск на основе семантической структуры -- другое дело.
Если у «Apple» как сущности есть уникальный идентификатор,
его никогда не спутают с «apple» -- фруктом.
Если «Q3 2024» -- это поле времени,
оно механически отличается от «Q3 2023».

Не нужно вычислять сходство.
Совпадает или нет? Да или нет.

---

## Проблема 2: Чанки -- не единицы смысла

Вернёмся к первому шагу RAG.
«Разбить документы на чанки.»

Именно «разбить» -- проблема.

Когда документ разбивается на фрагменты по 500 токенов,
смысл обрезается на середине.
Абзац разделяется на два чанка.
Посылка и вывод аргумента оказываются в разных частях.

«Ли Сунсин с 12 кораблями вышел против 133 в битве при Мённяне» -- в чанке A,
а «учёные оспаривают эти цифры» -- в чанке B.
Если для запроса извлекается только чанк A,
информация о степени достоверности попадает в контекст уже утраченной.

Увеличить чанки? Они займут больше окна.
Уменьшить чанки? Больше контекста обрежется.
Добавить перекрытие? Окно тратится на дубликаты.

Как ни настраивай, фундаментальная проблема та же.
Разбиение текста на естественном языке по количеству токенов --
это разбиение смысла по количеству токенов.
У смысла есть свой естественный размер,
и деление его по не связанной с ним единице порождает проблемы.

В структурированном представлении единицы смысла явно определены.
Одно предикативное высказывание -- одно ребро.
Ребро не разрезается.
Поиск работает на уровне рёбер.
Смысл не обрезается посередине.

---

## Проблема 3: Качество результатов поиска неизвестно

RAG вернул 5 чанков.
Прежде чем поместить эти 5 в контекст, нужно задать вопросы.

Каков источник этой информации?
Какова дата привязки?
Насколько она достоверна?
Не противоречат ли эти 5 друг другу?

В чанках на естественном языке узнать это невозможно.

Источник может упоминаться где-то в чанке в виде текста -- а может и нет.
Временная привязка может быть где-то в документе или может быть утрачена при разбиении.
Для степени достоверности в естественном языке нет структурного слота, поэтому она почти всегда отсутствует.
Проверка противоречий требует чтения всех 5 чанков и рассуждений над ними.

В итоге оценку качества приходится делегировать LLM.
Вы используете RAG, чтобы снизить затраты на вызовы LLM,
но вызываете LLM, чтобы проверить результаты RAG.

В структурированном представлении источник, время и достоверность -- это поля.
«Исключить утверждения без источника» -- одна строка запроса.
«Исключить информацию старше 2023 года» -- одно сравнение полей.
«Исключить достоверность ниже 0.5» -- одно числовое сравнение.
Вызов LLM не нужен.

---

## Фундаментальная предпосылка RAG

Корень этих трёх проблем -- одно.

RAG ищет естественный язык как естественный язык.

Документы -- естественный язык.
Чанки -- естественный язык.
Эмбеддинги -- статистические приближения естественного языка.
Результаты поиска -- естественный язык.
То, что попадает в контекст, -- естественный язык.

Неоднозначность естественного языка пронизывает весь пайплайн.

Поиск неточен, потому что неоднозначное содержание ищется в его неоднозначной форме.
Контекст теряется, потому что неоднозначное содержание разбивается по размеру, не связанному со смыслом.
Верификация невозможна, потому что из неоднозначного содержания нельзя извлечь информацию о качестве.

Большинство попыток улучшить RAG действуют в рамках этой предпосылки.

Использовать лучшую модель эмбеддингов. -- Статистическое приближение становится точнее, вот и всё.
Использовать лучшую стратегию разбиения. -- Позиции разрезов улучшаются, вот и всё.
Добавить reranker. -- Естественный язык прочитывается ещё раз, вот и всё.
Использовать гибридный поиск. -- Ключевые слова и сходство смешиваются, вот и всё.

Всё это работает.
Всё это остаётся в рамках естественного языка.
Ничего из этого не является фундаментальным решением.

---

## Условия для фундаментальной альтернативы

Чтобы выйти за пределы RAG, предпосылка должна измениться.
Не искать естественный язык как естественный язык,
а искать структурированные представления структурно.

Эта альтернатива должна удовлетворять трём условиям.

**Поиск по совпадению, а не по сходству.**
Не находить «то, что выглядит похоже»,
а находить «то, что совпадает».
Совпадает ли идентификатор? Входит ли в диапазон времени?
Да или нет. Не вероятность.

**Единица смысла -- единица поиска.**
Не разбивать по количеству токенов,
а хранить по предикативным высказываниям и искать по предикативным высказываниям.
Смысл не обрезается посередине.

**Метаданные встроены в структуру.**
Не нужно вызывать LLM для оценки качества результатов поиска.
Источник, время и достоверность -- это поля,
поэтому механическая фильтрация возможна.

Когда эти три условия выполнены,
поиск переходит от «угадывания правдоподобных кандидатов»
к «подтверждению того, что совпадает».

---

## RAG -- переходная технология

Это не уничижение RAG.

RAG был лучшим ответом в мире, где естественный язык был всем, что есть.
Когда документы были на естественном языке, знания хранились на естественном языке,
а LLM были инструментами для обработки естественного языка,
поиск естественного языка естественным языком был очевидным выбором.

И RAG действительно работает.
LLM с RAG значительно точнее, чем без него.
Это факт.

Но если предпосылка «мир, где естественный язык -- это всё» изменится,
позиция RAG тоже изменится.

Если существуют структурированные представления,
RAG становится фронтендом, который «принимает ввод на естественном языке и ищет по структурированному хранилищу».
Естественный язык -> структурированный запрос -> структурный поиск -> структурированные результаты -> контекст.

RAG не исчезает.
Его бэкенд меняется.
От эмбеддинг-поиска по сходству к поиску на основе семантической структуры.

---

## Итог

RAG -- текущий стандарт контекстной инженерии.
И у него три фундаментальных ограничения.

1. **Похожее != релевантное.** Эмбеддинг-сходство не гарантирует релевантности. «Выглядит похоже» и «является релевантным» -- разные вещи.
2. **Чанк != смысл.** Разбиение по количеству токенов обрезает смысл посередине. Посылки и выводы разделяются. Информация о достоверности теряется.
3. **Оценка качества невозможна.** Источник, время и достоверность извлечённых чанков невозможно определить механически. Для их оценки нужен вызов LLM.

Корень этих трёх проблем -- одно.
Поиск естественного языка как естественного языка.

Фундаментальная альтернатива -- сменить предпосылку.
Совпадение, а не сходство.
Предикативное высказывание, а не токенный чанк.
Встроенные метаданные, а не внешняя оценка.

RAG -- переходная технология.
Он был лучшим ответом в мире, где естественный язык был всем, что есть.
Когда эта предпосылка изменится, бэкенд RAG изменится.
