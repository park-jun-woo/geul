---
title: "왜 RAG로는 부족한가"
weight: 2
date: 2026-02-26T12:00:11+09:00
lastmod: 2026-02-26T12:00:11+09:00
tags: ["RAG", "검색", "임베딩"]
summary: "검색 결과가 관련 있어 보인다는 것과 관련 있다는 것은 다르다"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 검색 결과가 관련 있어 "보인다"는 것과 관련 "있다"는 것은 다르다.

---

## RAG는 현재의 표준이다

2024년 기준, 기업에서 LLM을 실무에 적용하는 가장 일반적인 방법은 RAG다.

Retrieval-Augmented Generation.
외부 문서를 검색해서 컨텍스트에 넣고, 그것을 참고하여 답하게 하는 것.

RAG는 작동한다.
LLM이 학습하지 않은 사내 문서를 참조할 수 있게 해준다.
최신 정보를 반영할 수 있게 해준다.
환각을 상당히 줄여준다.

RAG가 없었다면 LLM의 기업 도입은 훨씬 느렸을 것이다.
RAG는 존경받아야 할 기술이다.

그러나 RAG에는 근본적인 한계가 있다.
그 한계는 RAG를 더 잘 만들어서 해결되는 것이 아니다.
RAG의 전제 자체에서 오는 것이기 때문이다.

---

## RAG의 작동 원리

RAG의 핵심은 세 단계다.

**1단계: 문서를 청크로 자른다.**
PDF, 위키, 사내 문서를 일정 크기(보통 200~500토큰)로 분할한다.

**2단계: 각 청크를 임베딩 벡터로 변환한다.**
수백~수천 차원의 실수 벡터.
텍스트의 "의미"를 벡터 공간의 한 점으로 매핑한 것.

**3단계: 질의가 들어오면 유사한 벡터를 찾는다.**
질의도 벡터로 변환하고,
코사인 유사도가 높은 청크를 상위 5~20개 골라서
컨텍스트에 넣는다.

간단하고 우아하다.
그리고 여기에 세 가지 근본적인 문제가 있다.

---

## 문제 1: 유사함과 관련됨은 다르다

임베딩 유사도는 "두 텍스트가 비슷한 단어를 비슷한 맥락에서 사용하는가"를 측정한다.

이것은 관련성이 아니다.

예시.

질의: "애플의 2024년 3분기 매출은?"

임베딩 검색이 반환할 수 있는 청크들:
- "애플의 2024년 3분기 매출은 949억 달러를 기록했다." ✓ 관련 있음
- "애플의 2023년 3분기 매출은 818억 달러였다." △ 유사하지만 다른 시점
- "삼성전자의 2024년 3분기 매출은 79조원이었다." △ 유사하지만 다른 회사
- "애플 파이의 칼로리는 약 296kcal이다." × 키워드 겹침

임베딩 유사도로는 이 네 가지를 구분할 수 없다.
벡터 공간에서 "애플 매출"은 한 지점 근처에 모여 있다.
2023년인지 2024년인지, 애플인지 삼성인지는
벡터의 거리로 안정적으로 구분되지 않는다.

리랭커(reranker)를 추가하면 개선된다.
그러나 리랭커도 자연어 텍스트를 읽고 판단하는 것이므로
근본적인 모호함 문제는 남는다.

의미 구조 기반 검색은 다르다.
"애플"이라는 엔티티의 고유 식별자가 있으면
"사과"와 혼동되지 않는다.
"2024년 3분기"라는 시간 필드가 있으면
"2023년 3분기"와 기계적으로 구분된다.

유사도를 계산할 필요가 없다.
해당하는지 안 하는지, 예 아니오다.

---

## 문제 2: 청크는 의미의 단위가 아니다

RAG의 첫 번째 단계를 다시 보자.
"문서를 청크로 자른다."

이 "자른다"가 문제다.

문서를 500토큰 단위로 자르면,
의미의 중간에서 잘린다.
한 단락이 두 청크에 걸치고,
한 논증의 전제와 결론이 분리된다.

"이순신은 명량해전에서 12척의 배로 133척과 맞섰다."가 청크 A에 있고,
"이 수치에 대해 학계에서는 이견이 있다."가 청크 B에 있으면,
질의에 청크 A만 검색되었을 때
확신도 정보가 소실된 채로 컨텍스트에 들어간다.

청크 크기를 키우면? 윈도우를 더 많이 차지한다.
청크 크기를 줄이면? 맥락이 더 잘린다.
오버랩을 주면? 중복으로 윈도우를 낭비한다.

어떻게 조정해도 근본 문제는 같다.
자연어 텍스트를 토큰 수로 자르는 것은
의미를 토큰 수로 자르는 것과 같다.
의미에는 고유한 크기가 있는데,
그것과 무관한 단위로 분할하니까 문제가 생긴다.

구조화된 표현에서는 의미의 단위가 명시적이다.
하나의 서술이 하나의 엣지다.
엣지는 분할되지 않는다.
검색은 엣지 단위로 이루어진다.
의미의 중간에서 잘릴 일이 없다.

---

## 문제 3: 검색된 것의 품질을 알 수 없다

RAG가 청크 5개를 반환했다.
이 5개를 컨텍스트에 넣기 전에 물어야 할 것들이 있다.

이 정보의 출처는 무엇인가.
이 정보는 언제 기준인가.
이 정보는 얼마나 확실한가.
이 5개가 서로 모순되지는 않는가.

자연어 청크에서는 이것을 알 수 없다.

출처가 청크 안에 자연어로 언급되어 있을 수도 있고 없을 수도 있다.
시점이 문서 어딘가에 있을 수도 있고, 청크가 잘리면서 소실되었을 수도 있다.
확신도는 자연어에 구조적 자리가 없으므로 거의 항상 없다.
모순 검사는 5개 청크를 전부 읽고 추론해야 가능하다.

결국 품질 판단을 LLM에게 시켜야 한다.
LLM 호출 비용을 줄이려고 RAG를 쓰는데,
RAG 결과를 검증하기 위해 LLM을 호출한다.

구조화된 표현에서는 출처, 시점, 확신도가 필드에 있다.
"출처 없는 서술은 제외"가 쿼리 한 줄이다.
"2023년 이전 정보는 제외"가 필드 비교 한 번이다.
"확신도 0.5 미만은 제외"가 숫자 비교 한 번이다.
LLM을 호출할 필요가 없다.

---

## RAG의 근본 전제

이 세 가지 문제의 뿌리는 하나다.

RAG는 자연어를 자연어인 채로 검색한다.

문서가 자연어이고,
청크가 자연어이고,
임베딩이 자연어의 통계적 근사이고,
검색 결과가 자연어이고,
컨텍스트에 들어가는 것이 자연어다.

자연어의 모호함이 파이프라인 전체를 관통한다.

모호한 것을 모호한 채로 검색하니까 검색이 부정확하고,
모호한 것을 의미와 무관한 크기로 자르니까 맥락이 손실되고,
모호한 것에서 품질 정보를 추출할 수 없으니까 검증이 불가능하다.

RAG를 개선하는 대부분의 시도는 이 전제 안에서 이루어진다.

더 좋은 임베딩 모델을 쓴다. → 통계적 근사가 정교해질 뿐이다.
더 좋은 청킹 전략을 쓴다. → 자르는 위치가 나아질 뿐이다.
리랭커를 추가한다. → 자연어를 한 번 더 읽는 것일 뿐이다.
하이브리드 검색을 쓴다. → 키워드와 유사도를 섞는 것일 뿐이다.

전부 효과가 있다.
전부 자연어의 틀 안에 있다.
전부 근본적이지 않다.

---

## 근본적 대안의 조건

RAG의 한계를 넘으려면 전제를 바꿔야 한다.
자연어를 자연어인 채로 검색하는 것이 아니라,
구조화된 표현을 구조적으로 검색하는 것.

이 대안이 갖춰야 할 조건은 세 가지다.

**유사도가 아니라 해당 여부로 검색한다.**
"비슷해 보이는 것"을 찾는 것이 아니라
"해당하는 것"을 찾는다.
식별자가 일치하는가. 시간 범위에 포함되는가.
예 아니오. 확률이 아니다.

**의미의 단위가 검색의 단위다.**
토큰 수로 분할하는 것이 아니라
서술 단위로 저장하고 서술 단위로 검색한다.
의미의 중간에서 잘리지 않는다.

**메타데이터가 구조에 내장되어 있다.**
검색 결과의 품질을 판단하기 위해 LLM을 호출할 필요가 없다.
출처, 시점, 확신도가 필드에 있으므로
기계적 필터링이 가능하다.

이 세 조건이 충족되면
검색은 "그럴듯한 후보를 추측하는 것"에서
"해당하는 것을 확정하는 것"으로 바뀐다.

---

## RAG는 과도기의 기술이다

이것은 RAG를 폄하하는 것이 아니다.

RAG는 자연어밖에 없는 세계에서 최선의 답이었다.
문서가 자연어이고, 지식이 자연어로 저장되어 있고,
LLM이 자연어를 처리하는 도구인 상황에서
자연어를 자연어로 검색하는 것은 당연한 선택이었다.

그리고 RAG는 실제로 작동한다.
RAG 없는 LLM보다 RAG 있는 LLM이 훨씬 정확하다.
이것은 사실이다.

그러나 "자연어밖에 없는 세계"라는 전제가 바뀌면
RAG의 위치도 바뀐다.

구조화된 표현이 존재하면,
RAG는 "자연어 입력을 받아 구조화된 저장소에서 검색하는" 프론트엔드가 된다.
자연어 → 구조화된 쿼리 → 구조적 검색 → 구조화된 결과 → 컨텍스트.

RAG가 사라지는 것이 아니다.
RAG의 백엔드가 바뀌는 것이다.
임베딩 유사도 검색에서 의미 구조 기반 검색으로.

---

## 요약

RAG는 현재 컨텍스트 엔지니어링의 표준이다.
그리고 세 가지 근본적 한계가 있다.

1. **유사함 ≠ 관련됨.** 임베딩 유사도는 관련성을 보장하지 않는다. "비슷해 보이는 것"과 "관련 있는 것"은 다르다.
2. **청크 ≠ 의미.** 토큰 수로 분할하면 의미의 중간에서 잘린다. 전제와 결론이 분리되고, 확신도 정보가 소실된다.
3. **품질 판단 불가.** 검색된 청크의 출처, 시점, 확신도를 기계적으로 알 수 없다. 판단하려면 LLM을 호출해야 한다.

세 문제의 뿌리는 하나다.
자연어를 자연어인 채로 검색한다는 것.

근본적 대안은 전제를 바꾸는 것이다.
유사도가 아니라 해당 여부로.
토큰 청크가 아니라 서술 단위로.
외부 판단이 아니라 내장된 메타데이터로.

RAG는 과도기의 기술이다.
자연어밖에 없던 세계의 최선이었다.
그 전제가 바뀌면, RAG의 백엔드가 바뀐다.
