---
title: "왜 임베딩 벡터로는 안 되는가"
weight: 11
date: 2026-02-26T12:00:18+09:00
lastmod: 2026-02-26T12:00:18+09:00
tags: ["임베딩", "벡터", "화이트박스"]
summary: "임베딩 벡터를 정렬하면 모델이 부서진다. 부서지지 않게 하려면 모델을 다시 만들어야 한다. 블랙박스의 내부가 아니라 외부에 투명한 층이 필요하다."
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 벡터는 연산에는 좋지만 해석이 불가능하다. 블랙박스의 내부를 투명하게 만들 수는 없다.

---

## 임베딩 벡터는 놀라운 기술이다

"왕 - 남자 + 여자 = 여왕."

word2vec이 이것을 보여주었을 때 세계가 놀랐다.
단어를 수백 차원의 벡터로 표현하면
의미의 관계가 벡터의 연산으로 나타난다.

임베딩 벡터는 LLM의 기반이다.
트랜스포머의 모든 것은 벡터 연산이다.
토큰이 벡터가 되고,
어텐션이 벡터 간의 유사도를 계산하고,
출력이 벡터에서 토큰으로 변환된다.

유사한 의미는 가까운 벡터.
다른 의미는 먼 벡터.
검색은 벡터 유사도 계산.
분류는 벡터 공간의 경계 설정.

임베딩 벡터가 없었다면 현재의 AI는 존재하지 않는다.

그렇다면 임베딩 벡터로 지식을 표현하면 되는 것 아닌가.
벡터를 직접 정렬하고, 구조화하고, 해석 가능하게 만들면.

안 된다.
이것을 가장 확실하게 아는 방법은 시도해보는 것이다.

---

## AILEV: 시도했다

GEUL 프로젝트는 처음에 AILEV라는 이름으로 시작했다.

AI Language Embedding Vector.

이름 자체가 목적을 말한다.
임베딩 벡터를 직접 다루는 AI 언어.

구상은 이랬다.

512차원의 벡터로 의미를 표현한다.
벡터의 구간에 역할을 부여한다.
앞 128차원은 엔티티, 다음 128차원은 관계, 다음 128차원은 속성, 나머지는 메타데이터.
RGBA가 색을 4개의 채널로 분해하듯이, 의미를 차원의 구간으로 분해한다.

BERT를 학습시켜서 자연어를 이 구조화된 벡터로 변환한다.
"서울은 한국의 수도"가 입력되면
엔티티 구간에 서울 벡터, 관계 구간에 수도 벡터, 속성 구간에 한국 벡터가 나온다.

벡터이므로 연산이 가능하다.
유사도 검색이 가능하다.
차원을 축소하면 우아한 열화도 된다.
512차원에서 256차원으로 줄이면 정밀도는 떨어지지만 핵심 의미는 유지된다.

우아했다. 이론적으로.

---

## 왜 실패하는가

### 벡터를 임의로 정렬하면 모델이 부서진다

LLM의 임베딩 벡터는 학습의 결과다.

수십억 개의 텍스트를 읽으면서
모델이 스스로 최적화한 내부 표현이다.
각 차원이 무엇을 의미하는지는 모델이 결정한 것이다.
사람이 결정한 것이 아니다.

"앞 128차원은 엔티티"라고 정하면 어떻게 되는가.

모델이 학습한 벡터 공간에서는
엔티티 정보가 앞 128차원에 있지 않다.
768차원 전체에 분산되어 있다.
관계 정보도, 속성 정보도, 시제 정보도 전부 섞여 있다.

이것은 설계의 실수가 아니라 학습의 본질이다.
역전파(backpropagation)는
과제 수행에 최적인 벡터 배치를 찾는다.
해석 가능한 배치를 찾지 않는다.
최적과 해석 가능은 같은 것이 아니다.

벡터를 강제로 재배치하면 — "엔티티는 여기, 관계는 저기" —
모델이 학습한 통계적 관계가 깨진다.
성능이 떨어진다.

### 깨지 않게 정렬하는 것은 모델을 다시 만드는 것이다

그러면 처음부터 "앞 128차원은 엔티티"라는 제약 조건을 걸고 학습시키면 되지 않는가.

된다. 이론적으로.
그러나 이것은 임베딩 벡터를 정렬하는 것이 아니라
새로운 모델 아키텍처를 설계하는 것이다.

학습 데이터가 필요하다. 수십억 토큰.
학습 인프라가 필요하다. 수천 GPU.
학습 시간이 필요하다. 수개월.
그리고 그 모델이 기존 LLM만큼 잘 작동한다는 보장이 없다.

일이 너무 크다.

"벡터를 정렬해서 해석 가능하게 만든다"는 문제가
"LLM을 처음부터 다시 만든다"는 문제로 바뀌었다.
이것은 문제의 해결이 아니라 문제의 확대다.

### 해석이 불가능하다

설령 구조화된 벡터를 만들었다고 하자.
512차원의 벡터가 있다.
앞 128차원이 엔티티라고 하자.

엔티티 구간의 값이 `[0.23, -0.47, 0.81, 0.12, ...]`이다.

이것이 "삼성전자"인지 "현대자동차"인지
어떻게 아는가.

가장 가까운 벡터를 찾아야 한다.
벡터 데이터베이스에서 유사도를 계산해야 한다.
그래서 "아마 삼성전자일 것이다"라는 확률적 답을 얻는다.

"아마."

벡터는 본질적으로 연속적이다.
삼성전자와 SK하이닉스의 벡터 사이에
무한한 중간 벡터가 존재한다.
그 중간 벡터가 무엇을 의미하는지 아무도 모른다.

이것은 기술적 한계가 아니라 수학적 본질이다.
연속 공간에서 이산적 의미를 표현하면
경계가 모호해진다.
모호함은 [자연어의 문제](/ko/why/natural-language-hallucination/)였다.
벡터로 바꿨는데 모호함이 다시 나타났다.

형태만 바뀌었다.
자연어에서는 단어의 모호함.
벡터에서는 좌표의 모호함.

---

## 화이트박스 원칙

여기서 근본적인 설계 원칙의 문제가 드러난다.

임베딩 벡터는 블랙박스다.
768차원의 실수 벡터를 보고 그 안에 무슨 정보가 어떻게 담겨 있는지
사람이 알 수 없다.
모델도 설명하지 못한다.

이것은 불편한 특성이 아니라 존재론적 속성이다.
벡터가 작동하는 이유가 바로 이것이기 때문이다.
사람이 설계하지 않은 방식으로 정보를 배치하기 때문에
사람이 설계한 것보다 더 잘 작동한다.
해석 불가능성은 버그가 아니라 피처다.

그러나 AI의 컨텍스트로 사용되는 지식은 반대의 요구를 받는다.

출처가 무엇인지 알아야 한다.
시점이 언제인지 알아야 한다.
확신도가 얼마인지 알아야 한다.
무엇에 대한 서술인지 알아야 한다.
두 서술이 같은 대상에 대한 것인지 알아야 한다.

전부 "알아야 한다"다. 전부 해석 가능성의 요구다.

블랙박스인 벡터로 화이트박스의 요구를 충족하는 것은
모순이다.

---

## 전환의 논리

AILEV에서 GEUL로의 전환은 포기가 아니었다.
문제의 재정의였다.

**원래 문제:** LLM은 블랙박스다. 내부를 투명하게 만들자.
→ 임베딩 벡터를 해석 가능하게 정렬하자.
→ 벡터를 건드리면 모델이 부서진다.
→ 부서지지 않게 하려면 모델을 다시 만들어야 한다.
→ 막다른 길.

**재정의된 문제:** 블랙박스의 내부를 투명하게 만들 필요가 없다. 외부에 투명한 층을 만들자.
→ LLM의 내부는 건드리지 않는다.
→ LLM의 밖에, 해석 가능한 표현 체계를 만든다.
→ LLM은 그 표현 체계를 읽고 쓸 수 있다. 토큰이니까.
→ 인공언어.

벡터가 아니라 언어.
연속적이 아니라 이산적.
해석 불가능이 아니라 해석이 유일한 목적.
모델의 내부가 아니라 모델의 외부.

AILEV의 "Embedding Vector"가 빠지고
GEUL의 "글"이 된 이유가 이것이다.

---

## 벡터는 연산에, 언어는 표현에

이것은 임베딩 벡터를 부정하는 것이 아니다.

벡터는 연산에 최적화되어 있다.
유사도 계산, 클러스터링, 분류, 검색.
벡터가 하는 일을 언어가 대체할 수 없다.

언어는 표현에 최적화되어 있다.
엔티티의 정체성, 관계의 기술, 메타데이터의 내장, 해석 가능성.
언어가 하는 일을 벡터가 대체할 수 없다.

둘은 다른 층위의 도구다.

LLM의 내부에서는 벡터가 작동한다. 블랙박스다. 그래야 한다.
LLM의 외부에서는 언어가 작동한다. 화이트박스다. 그래야 한다.

문제는 이 두 층위를 혼동한 데서 시작했다.
벡터로 언어의 일을 시키려 했다.
블랙박스에게 화이트박스의 역할을 맡기려 했다.

각자의 자리가 있다.

---

## 요약

임베딩 벡터는 LLM의 기반이며 놀라운 기술이다.
그러나 지식 표현의 수단으로는 근본적 한계가 있다.

GEUL은 AILEV(AI Language Embedding Vector)로 시작했다.
벡터를 직접 정렬하여 해석 가능하게 만들려 했다.
실패했다. 두 가지 이유로.

벡터를 임의로 정렬하면 모델이 학습한 관계가 깨진다.
깨지 않게 정렬하려면 모델을 처음부터 다시 만들어야 한다. 일이 너무 크다.

그리고 설령 성공하더라도 벡터는 해석이 불가능하다.
연속 공간에서 이산적 의미의 경계는 모호하다.
블랙박스에게 화이트박스의 역할을 맡길 수 없다.

전환의 논리:
블랙박스의 내부를 투명하게 만들려 했다.
내부를 건드리면 부서진다.
내부를 건드리지 않고, 외부에 투명한 층을 따로 만들자.
벡터가 아니라 언어. 모델의 내부가 아니라 모델의 외부.

벡터는 연산에, 언어는 표현에.
각자의 자리가 있다.
