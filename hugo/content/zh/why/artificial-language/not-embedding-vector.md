---
title: "为什么嵌入向量行不通"
weight: 11
date: 2026-02-26T12:00:18+09:00
lastmod: 2026-02-26T12:00:18+09:00
tags: ["嵌入", "向量", "白盒"]
summary: "重新排列嵌入向量会破坏模型。要避免破坏就必须重建模型。我们需要的不是让黑盒内部变透明，而是在外部构建一个透明层。"
author: "Junwoo Park"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 向量擅长计算，但无法解读。你不可能让黑盒的内部变得透明。

---

## 嵌入向量是一项了不起的技术

"国王 - 男人 + 女人 = 女王。"

当word2vec展示出这个结果时，全世界为之震惊。
将词语表示为数百维的向量，
语义关系就能通过向量运算体现出来。

嵌入向量是LLM的基础。
Transformer中的一切都是向量运算。
词元变成向量，
注意力机制计算向量间的相似度，
输出从向量转换回词元。

相似的含义是相近的向量。
不同的含义是远离的向量。
搜索是向量相似度计算。
分类是向量空间中的边界划定。

如果没有嵌入向量，今天的AI就不会存在。

那么，用嵌入向量来表示知识不就行了吗？
直接对齐向量，赋予结构，使其可解读。

不行。
验证这一点最确切的方法就是亲自尝试。

---

## AILEV：我们尝试过

GEUL项目最初以AILEV为名启动。

AI Language Embedding Vector。

名字本身就说明了目标：
一种直接操作嵌入向量的AI语言。

构想是这样的：

用512维向量表示语义。
为向量的各区间赋予角色。
前128维用于实体，接下来128维用于关系，再128维用于属性，其余用于元数据。
就像RGBA将颜色分解为四个通道一样，将语义分解为维度区间。

训练BERT将自然语言转换为这种结构化向量。
当输入"首尔是韩国的首都"时，
实体区间产生首尔向量，关系区间产生首都向量，属性区间产生韩国向量。

因为是向量，所以可以计算。
可以进行相似度搜索。
降低维度还能实现优雅退化。
从512维降到256维，精度下降但核心语义得以保留。

很优雅。理论上。

---

## 为什么会失败

### 任意重排向量会破坏模型

LLM的嵌入向量是训练的产物。

读过数十亿文本之后，
模型自行优化出的内部表示。
每个维度的含义是模型决定的，
不是人决定的。

如果你规定"前128维是实体"，会怎样？

在模型学习的向量空间中，
实体信息并不位于前128维。
它分布在全部768个维度中。
关系信息、属性信息、时态信息——全部混合在一起。

这不是设计失误，而是学习的本质。
反向传播寻找的是
对任务最优的向量排列，
而不是可解读的排列。
最优和可解读并不是一回事。

如果你强行重排向量——"实体在这里，关系在那里"——
模型学到的统计关系就会被打破。
性能会下降。

### 不破坏地重排等于重建模型

那么从一开始就加上"前128维是实体"的约束来训练呢？

可以。理论上。
但这不是在对齐嵌入向量，
而是在设计一个新的模型架构。

需要训练数据。数十亿个词元。
需要训练基础设施。数千块GPU。
需要训练时间。数月。
而且无法保证新模型能和现有LLM一样好用。

工程量太大了。

"对齐向量使其可解读"这个问题
变成了"从头重建一个LLM"的问题。
这不是在解决问题，而是在放大问题。

### 无法解读

假设你确实创建了一个结构化向量。
一个512维的向量。
假设前128维是实体。

实体区间的值是 `[0.23, -0.47, 0.81, 0.12, ...]`。

你怎么知道这是"三星电子"还是"现代汽车"？

你必须找到最近的向量。
必须在向量数据库中计算相似度。
然后得到一个概率性的答案："大概是三星电子。"

"大概。"

向量本质上是连续的。
在三星电子和SK海力士的向量之间，
存在无穷多个中间向量。
没有人知道那些中间向量意味着什么。

这不是技术限制，而是数学本质。
在连续空间中表示离散语义，
边界就会变得模糊。
模糊性正是[自然语言的问题](/zh/why/natural-language-hallucination/)。
换成了向量，模糊性又回来了。

只是形式变了。
在自然语言中，是词语的模糊。
在向量中，是坐标的模糊。

---

## 白盒原则

至此，根本性的设计问题显现出来。

嵌入向量是黑盒。
看着一个768维的实数向量，
没有人能知道其中编码了什么信息、如何编码。
模型自身也解释不了。

这不是令人不便的特性，而是存在论层面的属性。
向量之所以有效，正是因为这一点。
因为它以人类未设计过的方式排列信息，
所以比人类设计的任何方案都运行得更好。
不可解读性不是缺陷，而是特性。

然而，作为AI上下文使用的知识有着相反的需求。

需要知道来源。
需要知道时间点。
需要知道置信度。
需要知道描述的对象是什么。
需要知道两个描述是否指向同一实体。

每一项都是"需要知道"。每一项都要求可解读性。

用黑盒的向量去满足白盒的需求，
这是矛盾的。

---

## 转向的逻辑

从AILEV到GEUL的转向不是放弃，
而是对问题的重新定义。

**原始问题：** LLM是黑盒。让我们把内部变透明。
→ 通过对齐嵌入向量使其可解读。
→ 动了向量，模型就坏了。
→ 不坏就得重建模型。
→ 死路一条。

**重新定义的问题：** 不需要让黑盒内部变透明。在外部建一个透明层。
→ 不碰LLM的内部。
→ 在LLM外部创建一套可解读的表示体系。
→ LLM可以读写这套体系。因为它是词元。
→ 人工语言。

不是向量，而是语言。
不是连续的，而是离散的。
不是不可解读，而是以解读为唯一目的。
不是在模型内部，而是在模型外部。

AILEV中的"Embedding Vector"被去掉了，
变成了GEUL——意为"文字"。原因就是这个。

---

## 向量用于计算，语言用于表达

这并不是否定嵌入向量。

向量针对计算进行了优化。
相似度搜索、聚类、分类、检索。
语言无法替代向量的工作。

语言针对表达进行了优化。
实体身份、关系描述、元数据内嵌、可解读性。
向量无法替代语言的工作。

它们是不同层次的工具。

在LLM内部，向量运行。黑盒。理当如此。
在LLM外部，语言运行。白盒。理当如此。

问题始于混淆了这两个层次。
我们试图让向量去做语言的事。
我们试图让黑盒承担白盒的角色。

各有各的位置。

---

## 总结

嵌入向量是LLM的基础，是一项了不起的技术。
但作为知识表示手段，存在根本性的局限。

GEUL始于AILEV（AI Language Embedding Vector）。
目标是直接对齐向量使其可解读。
失败了。原因有二。

任意对齐向量会打破模型学到的关系。
不打破就得从头重建模型。工程量太大。

即使成功了，向量也无法解读。
在连续空间中，离散语义的边界是模糊的。
不能让黑盒承担白盒的角色。

转向的逻辑：
曾试图让黑盒内部变透明。
碰了内部就会坏。
不碰内部，在外部另建一个透明层。
不是向量而是语言。不是模型内部而是模型外部。

向量用于计算，语言用于表达。
各有各的位置。
