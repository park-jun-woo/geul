---
title: "为什么自然语言会产生幻觉?"
weight: 8
date: 2026-02-26T12:00:16+09:00
lastmod: 2026-02-26T12:00:16+09:00
tags: ["自然语言", "幻觉", "歧义"]
summary: "幻觉不是LLM的bug——它是自然语言四大结构性缺陷的必然产物：歧义、来源缺失、置信度缺失、时间缺失。更大的模型无法修复它。"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 幻觉不是bug。只要使用自然语言，它就是结构性必然。

---

## 自然语言的奇迹

十万年前，口语出现了。灵长类通过互相梳毛所能维持的社会关系上限大约是150人。语言打破了这个天花板。一旦一个人可以同时对多人说话，一种新规模的社会——部落——就成为可能。

一万年前，农业创造了粮食盈余，人们聚集在一起形成城市。五千年前，美索不达米亚的某人在湿润的泥板上压下楔形符号。那是为了记录谷物库存。文字的诞生。口语消逝，但记录留存。一旦记录留存，官僚制度就成为可能，法律成为可能，国家成为可能。

口语创造了部落。文字创造了国家。

自然语言是人类创造的最伟大的技术。不是火的发现，不是轮子的发明，不是半导体的发明。使所有这些成为可能的是自然语言。因为自然语言的存在，知识得以传递，合作得以发生，死者的思想得以被生者继承。数万年来，自然语言是所有人类文明的媒介。

而现在，这种伟大的自然语言已经成为AI时代的瓶颈。

---

## 叫做"幻觉"的误解

当AI说出虚假的东西时，我们称之为"幻觉"。

这个名称暗含了几层意思。
暗示幻觉是异常的。
暗示它可以被修复。
暗示更好的模型可以解决它。

这是一种误解。

幻觉不是LLM的bug。
幻觉是只要自然语言被用作AI的推理语言，
就无法避免的结构性必然。

无论模型扩展到多大，
无论数据扩展到多少，
无论RLHF多么精细，
只要输入是自然语言，输出也是自然语言，
幻觉就不会消失。

让我解释原因。

---

## 自然语言的四个结构性缺陷

自然语言是为人类之间的交流而进化的。
它在此过程中获得的四个特征
在AI推理中成为致命缺陷。

---

### 缺陷1：歧义

"He went to the bank."

"bank"是金融机构还是河岸？
"he"是谁？
他什么时候去的？

人类用上下文来消解歧义。
对话的脉络、说话者的表情、共同的背景知识。

AI只有文本。
仅凭文本无法完全消解歧义。
如果无法消解，AI就猜测。
猜测有时是错的。
当错误的猜测被自信地输出，那就是幻觉。

---

### 缺陷2：来源缺失

"李舜臣以12艘船击败了133艘。"

这句话没有来源。

谁提出了这个主张？
哪些历史记录支持？
学术界对这些数字有争议吗？

自然语言没有结构化的元数据空间。
要包含来源，就必须把句子变长，
而变长又会模糊重点。
所以在大多数自然语言句子中，来源被省略了。这个问题在[为什么是主张而非事实？](/zh/why/claims-not-facts/)中有更深入的探讨。

LLM在数十亿这样的句子上训练。
来源被省略的主张混合在一起
形成一锅巨大的统计汤。

在那锅汤中追溯"12"这个数字的依据
在原理上是不可能的。
既然依据不可追溯，无依据的数字也可以被编造。
那就是幻觉。

---

### 缺陷3：置信度缺失

"地球是圆的。"
"暗能量占宇宙的68%。"
"明天会下雨。"

这三句话的置信度水平完全不同。

第一句是压倒性共识。
第二句是当前最佳估计，但理论可能改变。
第三句是概率性预测。

然而在自然语言中，三句话的语法结构完全相同。
主语 + 谓语。陈述句。句号。

自然语言无法在结构上表达"这有多确定"。
有"也许"、"几乎可以确定"、"可能"这样的副词手段，
但它们是可选的、不精确的，通常被省略。

LLM以相同的置信度学习所有句子。
模型内部没有办法区分
"地球是圆的"和"暗能量是68%"之间的置信度差异。

所以它把估计当事实陈述，
把假说当既定观点陈述，
把不确定的事确定地陈述。
那就是幻觉。

---

### 缺陷4：时间上下文缺失

"特斯拉的CEO是Elon Musk。"

截至什么时候？

2024年，这是正确的。
2030年，谁知道。
如果不指定写作时间，
这句话的有效期就无法确定。

大多数自然语言句子省略时间上下文。
"现在时"可以表示"此刻"
也可以表示"一般而言"。

LLM将2020年的文章和2024年的文章作为相同数据学习。
由于时间信息在结构上未被保留，
它们把过去的事实当成现在的来陈述，
或者混淆不同时期的信息。
那就是幻觉。

---

## 四个缺陷的汇流

当这四个缺陷汇聚时，幻觉爆炸性地升级。

让我们分析一个LLM输出。

> "李舜臣以12艘船摧毁了330艘日本船只，
> 后来在露梁海战中阵亡，留下遗言'勿告知我的死讯。'"

在这句话中：

**歧义：**"摧毁"具体是什么意思？击沉？击溃？部分损坏？

**来源缺失：**12和330的依据是什么？不同历史记录引用不同数字——跟随的是哪一个？

**置信度缺失：**"勿告知我的死讯"是经历史证实的遗言，还是后来的口传？两者的置信度不同，却以相同的陈述句列出。

**时间上下文缺失：**这些信息反映的是哪个时间点的学术共识？

LLM用"最合理的token序列"填充所有这些歧义。
合理性不是准确性。
两者之间的差距就是幻觉。

---

## 为什么更大的模型无法解决这个问题

"GPT-5出来后幻觉不会减少吗？"

会减少。但不会消失。

更大的模型从更多数据中学习更精密的模式。
所以"合理性"的准确度上升了。

但根本问题没有改变。

只要输入是自然语言，歧义就存在。
只要训练数据是自然语言，来源就丢失。
只要输出是自然语言，置信度就无法表达。
只要结构中缺少时间信息，时间就混乱。

即使将模型扩大100倍，
自然语言的结构性缺陷不会增长100倍——
但它们也不会达到零。

这不是分辨率的问题。这是媒介的问题。

无论多少提高黑白照片的分辨率，颜色不会出现。
无论多少提高自然语言的精度，
来源、置信度和时间上下文不会出现在结构中。

如果你想要颜色，你需要彩色胶片。
如果你想消除幻觉，你需要一种不同的语言。

---

## 结构性解决方案的条件

要解决这四个缺陷，语言本身的结构必须不同。

**歧义 --> 显式结构化。**
当"He went to the bank"被转换为结构化语言时，
"he"被解析为特定的实体SIDX，
"bank"被解析为金融机构或河岸的SIDX。
如果无法解析，"未解析"被显式声明。
要么消解歧义，要么记录歧义存在的事实。

**来源缺失 --> 嵌入来源。**
每个叙述在结构上包含来源实体。
"谁提出了这个主张"是叙述的一部分。
这不是可选的。如果字段为空，标记为空。

**置信度缺失 --> 嵌入置信度。**
每个动词边都有置信度字段。
"确定"、"估计"、"假设"
作为动词修饰语在结构上被指定。

**时间上下文缺失 --> 嵌入时间上下文。**
每个叙述包含时间上下文。
"这个叙述截至何时"始终被指定。

在自然语言中被省略的内容
在结构化语言中作为结构的一部分存在。

当省略不可能时，幻觉的空间就缩小了。[为什么明确化是必要的](/zh/why/clarification/)解释了这一原理。
当不能无依据地说话时，无依据的陈述就不会产生。

---

## 幻觉的终结在于替换语言

让我们看看当前减少幻觉的方法。

**RAG (检索增强生成)：**检索外部文档并作为上下文提供。有效，但检索到的文档也是自然语言，所以歧义、来源缺失和置信度缺失的问题原封不动地跟随。[为什么RAG是不够的](/zh/why/rag-not-enough/)详细探讨了这一局限。

**RLHF：**训练模型在不确定时说"我不知道"。减少幻觉的频率，但不能解决自然语言缺乏置信度结构的根本问题。

**Chain-of-Thought：**用自然语言记录推理过程。方向正确，但记录的媒介是自然语言，所以继承了同样的缺陷。

所有这些方法都试图在自然语言的框架内减轻幻觉。
它们有效。但不是根本性的。

根本性的解决方案是将自然语言从AI内部移除。

与用户的接口保持自然语言。
人类继续用自然语言说话，用自然语言接收回答。

但AI内部进行推理、记录和验证的语言
必须是自然语言以外的东西。

一种来源在结构中的语言。
一种置信度在结构中的语言。
一种时间上下文在结构中的语言。
一种歧义被显式处理的语言。

口语创造了部落。
文字创造了国家。
第三种语言将创造什么？

幻觉的终结不在于更大的模型
而在于更好的语言。

---

## 总结

幻觉源于自然语言的四个结构性缺陷。

1. **歧义：**没有上下文无法消解。AI猜测，而猜测是错的。
2. **来源缺失：**主张的依据丢失。无依据的组合被编造。
3. **置信度缺失：**事实和估计用相同的语法表达。AI无法区分。
4. **时间上下文缺失：**不同时期的信息被混淆。

更大的模型减少幻觉但无法消除它。
不更换媒介，结构性缺陷就会保留。

无论多少提高黑白胶片的分辨率，颜色不会出现。
如果你想要颜色，你必须更换胶片。
