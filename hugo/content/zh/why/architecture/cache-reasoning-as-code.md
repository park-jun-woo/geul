---
title: "为什么将推理缓存为代码?"
weight: 18
date: 2026-02-26T12:00:02+09:00
lastmod: 2026-02-26T12:00:02+09:00
tags: ["缓存", "推理", "代码"]
summary: "将一次推理转化为永久程序"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 将推理结晶为程序的理由

---

## 每次都从头思考的AI

想象你在教一位新同事如何在电子表格中创建数据透视表。

第一天，他问了。你花了三十分钟解释。
第二天，同一个同事问了同样的问题。你又花了三十分钟。
第三天，第四天——同样如此。

这正是今天的LLM的运作方式。

让GPT"用Python解析CSV"，模型动用数十亿参数从头推理。明天问同样的问题，或后天问，它每次都付出同样的成本。昨天的推理蒸发了。没有被记录，没有被复用，没有被积累。

这就是一个没有缓存运行的Web服务器。
一个反复做同一道考试题却不做笔记的学生。
而不积累经验的智能永远无法成长。

---

## LLM是编译器，不是运行时引擎

SEGLAM对这个问题提供了一个根本不同的答案。

**LLM不是执行每个请求的运行时引擎——
而是将推理结晶为代码的编译器。**

运作方式如下：

1. 当请求到达时，首先检查推理缓存。
2. **缓存命中：**相同或相似的推理过程已经被结晶为代码。不调用LLM。直接执行相应代码。快速、廉价、确定性。
3. **缓存未命中：**这是前所未见的推理类型。现在调用LLM。但LLM不生成"一个答案"——它生成**"产生答案的代码"。**这段代码被加入缓存。

下次收到类似请求？缓存命中。LLM可以继续休息。

---

## 与JIT编译的类比

这种架构是计算机科学中已被证明的模式的重新发现。

考虑JIT（即时编译器）。Java和JavaScript引擎最初通过解释器逐行执行代码。慢，但能用。当同一段代码路径被反复执行——"这是热路径"——引擎将该路径编译为原生机器码。从此之后，它直接运行而不经过解释器。

在SEGLAM中：

- **解释器 = LLM。**慢、昂贵、概率性，但能处理任何请求。
- **原生代码 = 缓存的推理代码。**快速、廉价、确定性。
- **JIT编译 = LLM在缓存未命中时生成代码的过程。**代价高昂，但只需发生一次。

正如JIT编译器优化"热路径"，
SEGLAM将"热推理"结晶为代码。

---

## 为什么缓存"代码"而不是"答案"?

这是关键。简单的响应缓存和SEGLAM的推理缓存有本质区别。

**响应缓存**存储的是"问：韩国的首都是什么？-> 答：首尔。"它只在问题完全匹配时命中。问"大韩民国的首都是什么？"就未命中。这是字典，不是智能。

**SEGLAM的推理缓存**存储的是"对于这类问题，通过这个程序构建答案"的代码。它结晶的不是具体的值，而是推理路径本身。因此，即使输入改变，同一类问题仍然命中。这是理解。这是成长。

打个比方：响应缓存背诵乘法表；推理缓存学会了如何乘法。

---

## 随时间发生的变化

这种设计最强大的特性是**时间站在它这一边。**

- **第1天：**缓存为空。几乎每个请求都是缓存未命中。LLM拼命工作。慢且昂贵。
- **第30天：**相当比例的常规推理模式被缓存。LLM调用减少。
- **第365天：**大多数请求是缓存命中。LLM只在遇到真正新型问题时被调用。系统快速、廉价且可预测。
- **之后：**缓存本身成为该领域的"结晶智能"。可移植、可验证、可积累的智力资产。

对LLM的依赖随时间递减。
系统效率随时间递增。
这条曲线永不逆转。

---

## 推理保存原则

这种方法最根本的原则是：

> "AI的推理过程不能丢弃——必须记录。"

推理缓存是这一哲学最直接的实现。

LLM执行一次的推理被结晶为结构化表示并存储。它不被丢弃。它被复用。被验证。被改进。被积累。

而且由于缓存的代码用清晰、结构化的语言描述：

- 你可以**追溯**给定程序为何被创建，
- 你可以**纠正**当某个程序被证明有误时，
- 你可以**替换**当发现更好的程序时。

不是在黑箱中每次调用都蒸发的推理，
而是在白箱上积累的智能。这是值得追求的AI愿景。

---

## 总结

| 传统LLM | SEGLAM |
|---------|--------|
| 每次请求都从头推理 | 缓存命中时执行缓存代码 |
| 推理结果蒸发 | 推理结晶为代码并积累 |
| 成本随使用量增长 | 成本随时间递减 |
| LLM = 运行时引擎 | LLM = 编译器 |
| 黑箱推理 | 可验证、可纠正、可替换的代码 |

每次请求都调用LLM就像坐飞机去隔壁家。
一旦铺好了路，从此可以步行。

SEGLAM就是铺路的系统。
