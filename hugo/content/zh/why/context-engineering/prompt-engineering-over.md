---
title: "为什么提示工程的时代结束了"
weight: 1
date: 2026-02-26T12:00:12+09:00
lastmod: 2026-02-26T12:00:12+09:00
tags: ["提示", "上下文", "工程"]
summary: "从怎么说到给什么看——游戏变了"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 为什么提示工程的时代结束了

### 从"怎么说"到"给什么看"——游戏变了。

---

### 提示工程作为一种职业

2023年，一个新职业出现了。

提示工程师。

"一步一步想。"
"你是一个有20年经验的专家。"
"让我先给你看一些例子。"

这样的句子成了价值数万美元的诀窍。同一个问题，取决于你怎么措辞，AI给出的答案截然不同。

提示工程确实有效。
一行Chain-of-Thought就能将数学分数提高20%。
一句角色分配就能改变专业知识的深度。
三个few-shot例子就能完全控制输出格式。

这不是炒作。这是真实的。
那为什么它正在结束？

---

### 它为什么有效：因为模型够笨

从第一性原理看提示工程为什么有效。很简单。

早期LLM不善于理解用户意图。
说"总结"，它会改写。
说"比较"，它会列举。

因为模型误读了意图，
精确传达意图的技能变得有价值。
提示工程本质上是"翻译"——
把人类意图翻译成LLM能理解的形式。

翻译要有价值，必须存在语言障碍。

---

### 变化是什么：模型变聪明了

从GPT-3.5到GPT-4。从Claude 2到Claude 3.5。
每一代，模型理解意图的能力都有了巨大提升。

说"总结"，它就总结。
说"比较"，它就比较。
即使不告诉它"一步一步想"，它也会自己将复杂问题分解成步骤。

语言障碍降低了。
翻译的价值缩水了。

2023年产生巨大差异的提示技术
到2025年只产生边际差异。
当模型足够聪明时，措辞越来越不重要。

那什么变得重要了？

---

### 上下文窗口：一条物理定律

LLM有一个物理约束。

上下文窗口。

无论是128K token还是1M token，它是有限的。
只有进入这个有限空间的信息才能影响推理。
窗口之外的信息，无论多重要，等于不存在。

这与模型大小无关。
即使有万亿参数，上下文窗口也是有限的。
即使训练数据覆盖整个互联网，上下文窗口也是有限的。

无论模型多聪明，
如果错误的信息进入上下文，它产生错误的答案。
如果无关的信息充满上下文，它错过重要的内容。
如果需要的信息不在上下文中，它等于不知道。

提示工程是"怎么说"的问题。
新的游戏是"给什么看"的问题。

这就是上下文工程。

---

### 类比：开卷考试

这里有一个类比来说明提示工程和上下文工程的区别。

提示工程是把考试题目出好。
不是写"选择下面的正确答案"，
而是写"逐步推导满足以下所有条件的答案"——
这样学生就能给出更好的答案。

上下文工程是开卷考试时你带哪些书的问题。
无论考试题目出得多好，
如果学生带错了书，就答不了题。
能带的书数量是有限的。
你带哪些书决定了你的成绩。

当模型笨的时候，题目格式（提示）重要。
当模型聪明的时候，参考资料（上下文）重要。

---

### 代理时代加速了这一转变

这一转变随着代理的出现而加速。

提示工程是人类每次手写的。
人类写问题、人类解释上下文、人类指定格式。

代理不同。
代理自主推理、调用工具、与其他代理协作。
在每一步，它们必须自己组装上下文。

一个代理调用了外部API并收到了数据。
这些数据需要进入下一轮推理的上下文。
哪些部分放进去，哪些部分留下？
之前的哪些推理结果保留，哪些丢弃？
另一个代理发送的信息能否信任？

人类无法每次都做这些决策。
要让代理自主运行，
上下文组装必须自动化。

提示工程是人类技能。
上下文工程必须是系统能力。

---

### 提示工程并没有消失

让我澄清一个误解。

我不是说提示工程正在变得毫无意义。
系统提示仍然重要。
输出格式规范仍然必要。
声明角色和约束条件仍然有效。

正在缩减的是提示工程所占的份额。

如果2023年70%的输出质量由提示决定，
到2025年，30%由提示决定，70%由上下文决定。

比例翻转了。

而这个趋势不会逆转。
模型会继续变得更聪明，
越聪明，措辞越不重要，
上下文越重要。

---

### 但上下文工程没有基础设施

这里是关键。

提示工程有工具。
提示模板、提示库、提示测试框架。
一整个生态系统被建立起来，系统化地管理"怎么说"。

上下文工程还没有这些。

看看目前实际中上下文是如何处理的。

RAG管道的块大小靠手工调整。
背景信息靠手工写入系统提示。
代理的记忆中存什么靠手工设计。
哪些搜索结果放入上下文靠手工决定。

全部都是手动的。

而所有这些手动工作的原材料是自然语言。
自然语言的文档被用自然语言切割，粘贴到自然语言的上下文中。

自然语言信息密度低。
没有来源。没有置信度。没有时间戳。
传达相同意义要消耗不必要的token。
没有办法自动化质量判断。

这类似于提示工程之前的时代。
提示工程起初也是手动的。
依赖个人的直觉和经验。
后来工具和方法论出现了，它才系统化。

上下文工程现在就处于那个之前的阶段。
问题已被认识，但基础设施不存在。

---

### 基础设施需要什么

要让上下文工程从手动工作变成系统，
至少需要以下几点。

**压缩。** 在同样的窗口中装入更多意义的方法。
剥离自然语言的语法粘合剂，只留下意义，
有效窗口大小就成倍增长——无需改变模型。

**索引。** 精确找到正确信息的方法。
基于语义结构搜索，而不是基于嵌入相似度。
一种搜索"苹果营收"不会拉出"苹果营养成分"的搜索。

**校验。** 机械地拒绝不符合规范的信息的方法。
就像Go编译器把未使用的变量当作错误，
没有来源的主张和没有时间戳的事实应该在进入上下文之前被过滤掉。
最便宜、最确定性的检查必须放在最前面。

**过滤。** 判断语义质量的方法。
如果说校验看的是形式，过滤看的是内容。
相关性、可靠性、新鲜度。这条信息对这轮推理真的需要吗？

**一致性。** 保证所选信息集内部一致性的方法。
单独看都好的信息组合在一起可能相互矛盾。
如果2020年的CEO和2024年的CEO同时进入上下文，
LLM会混乱。

**组装。** 优化窗口内位置和结构的方法。
同样的信息放在不同位置会获得不同的注意力权重。
放前面还是后面？如何分组？

**积累。** 让系统随时间学习和成长的方法。
缓存是对单个结果的复用。
积累是学习哪种上下文组合产生了好的结果，
以及增长知识库本身。

这七项是上下文工程基础设施的完整技术栈。

---

### 这不是关于某个特定工具

让我坦率地说。

谁来构建这个基础设施是一个开放问题。
一个工具可能解决所有问题，
也可能多个工具各自处理一层。

但基础设施是否需要不是一个开放问题。

上下文窗口是有限的，这是物理事实。
即使窗口扩大10倍，世界的信息增长更快。
自然语言信息密度低，这是结构性事实。
代理需要自动化上下文管理才能自主运行，这是逻辑必然。

就像提示工程需要工具一样，
上下文工程也需要工具。
但这一次，工具的性质不同。

提示工程的工具更接近文本编辑器。
上下文工程的工具更接近编译器。

压缩信息、索引它、校验它、过滤它、
检查一致性、优化位置、积累结果。
这不是编辑。这是工程。

所以它被称为上下文"工程"。

---

### 总结

提示工程在模型笨的时候有价值。
因为模型读不懂意图，所以精确传达意图的技能很重要。

随着模型变得更聪明，游戏变了。
从"怎么说"到"给什么看"。
从提示到上下文。

代理的出现加速了这一转变。
人类无法每次都组装上下文。
系统必须自己来。

但现在，上下文工程没有基础设施。
自然语言在被手动地剪切和粘贴。

所需的基础设施有七层：
压缩、索引、校验、过滤、一致性、组装、积累。

结束的不是提示工程的时代。
结束的是仅靠提示工程就够用的时代。
