---
title: "لماذا تخزين الاستدلال كشيفرة برمجية؟"
weight: 18
date: 2026-02-26T12:00:02+09:00
lastmod: 2026-02-26T12:00:02+09:00
tags: ["ذاكرة مؤقتة", "استدلال", "شيفرة"]
summary: "حوّل استدلالاً واحداً إلى إجراء دائم"
author: "Junwoo Park"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## حجة بلورة الاستدلال في إجراءات

---

## ذكاء اصطناعي يفكر من الصفر في كل مرة

تخيل أنك تُعلّم زميلاً مبتدئاً كيفية إنشاء جدول محوري في جدول بيانات.

في اليوم الأول يسأل. تقضي ثلاثين دقيقة في الشرح.
في اليوم الثاني، نفس الزميل يسأل نفس السؤال. تقضي ثلاثين دقيقة أخرى.
اليوم الثالث، الرابع -- نفس الشيء.

هذا بالضبط كيف تعمل نماذج LLM اليوم.

اطلب من GPT "تحليل ملف CSV بلغة Python"، وسيُحشد النموذج مليارات المعاملات للاستدلال من الصفر. اطرح نفس السؤال غداً، أو بعد غد، ويدفع نفس التكلفة في كل مرة. استدلال الأمس يتبخر. لا يُسجَّل، لا يُعاد استخدامه، لا يتراكم.

هذا خادم ويب يعمل بدون ذاكرة مؤقتة.
طالب يحل نفس مسألة الامتحان مراراً دون تدوين ملاحظات.
والذكاء الذي لا يراكم الخبرة لا يمكنه النمو أبداً.

---

## LLM مُترجِم، ليس محرك تشغيل

SEGLAM يقدم إجابة مختلفة جذرياً لهذه المشكلة.

**LLM ليس محرك تشغيل ينفذ كل طلب --
إنه مُترجِم يبلور الاستدلال في شيفرة برمجية.**

إليك كيف يعمل:

1. عند وصول طلب، تحقق من ذاكرة الاستدلال المؤقتة أولاً.
2. **إصابة الذاكرة المؤقتة:** عملية استدلال مطابقة أو مشابهة سبق أن بُلورت في شيفرة. لا يُستدعى LLM. تُنفَّذ الشيفرة المقابلة فوراً. سريعة، رخيصة، وحتمية.
3. **إخفاق الذاكرة المؤقتة:** هذا نوع استدلال لم يُشاهد من قبل. الآن يُستدعى LLM. لكن LLM لا يولّد "إجابة" -- بل يولّد **"شيفرة تُنتج الإجابة."** تُضاف هذه الشيفرة إلى الذاكرة المؤقتة.

عندما يأتي طلب مشابه في المرة القادمة؟ إصابة ذاكرة مؤقتة. يمكن لـ LLM أن يبقى نائماً.

---

## التشابه مع ترجمة JIT

هذه البنية هي إعادة اكتشاف لنمط أثبت نفسه بالفعل في علم الحاسوب.

تأمل مُترجِم JIT (Just-In-Time). محركات Java وJavaScript تنفذ الشيفرة سطراً بسطر عبر مفسّر في البداية. بطيء، لكنه يعمل. عندما يُنفَّذ نفس مسار الشيفرة مراراً -- "هذا مسار ساخن" -- يُترجم المحرك ذلك المسار إلى شيفرة آلة أصلية. من ذلك الحين، يعمل مباشرة دون المرور بالمفسّر.

في SEGLAM:

- **المفسّر = LLM.** بطيء ومكلف واحتمالي، لكنه قادر على التعامل مع أي طلب.
- **الشيفرة الأصلية = شيفرة الاستدلال المخزنة.** سريعة ورخيصة وحتمية.
- **ترجمة JIT = عملية توليد LLM للشيفرة عند إخفاق الذاكرة المؤقتة.** مكلفة، لكنها تحتاج أن تحدث مرة واحدة فقط.

كما يُحسّن مُترجِم JIT "المسارات الساخنة"،
يبلور SEGLAM "الاستدلال الساخن" في شيفرة.

---

## لماذا تخزين "شيفرة" بدلاً من "إجابات"؟

هذا هو لب الموضوع. ذاكرة مؤقتة بسيطة للاستجابات وذاكرة الاستدلال في SEGLAM مختلفتان جذرياً.

**ذاكرة الاستجابة المؤقتة** تخزّن "س: ما عاصمة كوريا؟ -> ج: سيول." تُصيب فقط عندما يتطابق السؤال تماماً. اسأل "ما عاصمة جمهورية كوريا؟" وتُخفق. هذا قاموس، ليس ذكاءً.

**ذاكرة الاستدلال في SEGLAM** تخزّن شيفرة تقول "لهذا النوع من الأسئلة، أنشئ الإجابة عبر هذا الإجراء." تُبلور ليس القيمة المحددة، بل مسار الاستدلال نفسه. لذلك، حتى عندما يتغير المُدخل، نفس نوع السؤال لا يزال يُصيب. هذا فهم. هذا نمو.

بتشبيه: ذاكرة الاستجابة المؤقتة تحفظ جدول الضرب؛ ذاكرة الاستدلال تتعلم كيف تضرب.

---

## ماذا يحدث مع مرور الوقت

أقوى خاصية لهذا التصميم هي أن **الزمن في صالحه.**

- **اليوم 1:** الذاكرة المؤقتة فارغة. تقريباً كل طلب إخفاق. يعمل LLM بجد. بطيء ومكلف.
- **اليوم 30:** جزء كبير من أنماط الاستدلال الروتينية مخزنة مؤقتاً. استدعاءات LLM تتناقص.
- **اليوم 365:** معظم الطلبات إصابات. يُستدعى LLM فقط لأنواع مشكلات جديدة حقاً. النظام سريع ورخيص وقابل للتنبؤ.
- **ما بعد ذلك:** الذاكرة المؤقتة نفسها تصبح "ذكاء متبلوراً" لمجالها. أصول فكرية قابلة للنقل والتحقق والتراكم.

الاعتماد على LLM يتناقص مع الوقت.
كفاءة النظام تزداد مع الوقت.
هذا المنحنى لا ينعكس أبداً.

---

## مبدأ حفظ الاستدلال

المبدأ الأساسي لهذا النهج هو:

> "عملية استدلال الذكاء الاصطناعي يجب ألا تُهمل -- بل يجب أن تُسجَّل."

ذاكرة الاستدلال المؤقتة هي التطبيق الأكثر مباشرة لهذه الفلسفة.

الاستدلال الذي يؤديه LLM مرة يُبلور في تمثيل منظم ويُخزَّن. لا يُهمل. يُعاد استخدامه. يُتحقق منه. يُحسَّن. يتراكم.

ولأن هذه الشيفرة المخزنة موصوفة بلغة واضحة ومنظمة:

- يمكنك **تتبع** سبب إنشاء إجراء معين،
- يمكنك **تصحيح** إجراء عندما يتبين أنه خاطئ،
- يمكنك **استبداله** عند اكتشاف إجراء أفضل.

ليس استدلالاً يتبخر داخل صندوق أسود مع كل استدعاء،
بل ذكاء يتراكم على صندوق أبيض. هذه هي رؤية الذكاء الاصطناعي التي تستحق السعي.

---

## ملخص

| LLM التقليدي | SEGLAM |
|-----------|--------|
| يستدل من الصفر في كل طلب | ينفذ شيفرة مخزنة عند الإصابة |
| نتائج الاستدلال تتبخر | الاستدلال يتبلور في شيفرة ويتراكم |
| التكلفة تتناسب مع الاستخدام | التكلفة تتناقص مع الوقت |
| LLM = محرك تشغيل | LLM = مُترجِم |
| استدلال صندوق أسود | شيفرة قابلة للتحقق والتصحيح والاستبدال |

استدعاء LLM لكل طلب كالسفر بالطائرة إلى البيت المجاور.
بمجرد أن تُعبّد طريقاً، يمكنك المشي من ذلك الحين.

SEGLAM هو النظام الذي يُعبّد الطرق.
