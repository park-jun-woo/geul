---
title: "なぜ推論をコードとしてキャッシュするのか?"
weight: 18
date: 2026-02-26T12:00:02+09:00
lastmod: 2026-02-26T12:00:02+09:00
tags: ["キャッシュ", "推論", "コード"]
summary: "一回の推論を永続的な手続きに変える"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 推論を手続きに結晶化させる理由

---

## 毎回ゼロから考えるAI

後輩にスプレッドシートでピボットテーブルの作り方を教えている場面を想像してみよう。

初日、質問される。30分かけて説明する。
二日目、同じ後輩が同じ質問をする。また30分。
三日目、四日目——同じことの繰り返し。

これがまさに今日のLLMの動作方法だ。

GPTに「PythonでCSVをパースして」と頼めば、モデルは数十億のパラメータを動員してゼロから推論する。明日同じ質問をしても、明後日しても、毎回同じコストを支払う。昨日の推論は蒸発する。記録されず、再利用されず、蓄積されない。

これはキャッシュなしで動くWebサーバーだ。
同じ試験問題をノートも取らずに繰り返し解く学生だ。
そして経験を蓄積しない知能は決して成長しない。

---

## LLMはコンパイラであり、ランタイムエンジンではない

SEGLAMはこの問題に根本的に異なる答えを提供する。

**LLMはすべてのリクエストを実行するランタイムエンジンではない——
推論をコードに結晶化するコンパイラだ。**

仕組みはこうだ：

1. リクエストが届いたら、まず推論キャッシュを確認する。
2. **キャッシュヒット：**同一または類似の推論プロセスがすでにコードに結晶化されている。LLMは起動しない。該当コードを即座に実行する。高速、低コスト、決定論的。
3. **キャッシュミス：**これは前例のない種類の推論だ。ここでLLMが起動する。しかしLLMは「答え」を生成するのではなく——**「答えを生成するコード」**を生成する。このコードがキャッシュに追加される。

次に同様のリクエストが来たら？キャッシュヒット。LLMは眠ったままでよい。

---

## JITコンパイルとのアナロジー

このアーキテクチャは、コンピュータサイエンスですでに実証されたパターンの再発見だ。

JIT（ジャストインタイム）コンパイラを考えてみよう。JavaとJavaScriptのエンジンは最初、インタプリタでコードを一行ずつ実行する。遅いが動く。同じコードパスが繰り返し実行されると——「これはホットパスだ」——エンジンはそのパスをネイティブマシンコードにコンパイルする。以後、インタプリタを通さず直接実行される。

SEGLAMでは：

- **インタプリタ = LLM。**遅く、高コストで、確率的だが、あらゆるリクエストに対応できる。
- **ネイティブコード = キャッシュされた推論コード。**高速、低コスト、決定論的。
- **JITコンパイル = キャッシュミス時にLLMがコードを生成するプロセス。**コストは高いが、一度だけで済む。

JITコンパイラが「ホットパス」を最適化するように、
SEGLAMは「ホットな推論」をコードに結晶化する。

---

## なぜ「答え」ではなく「コード」をキャッシュするのか?

ここが核心だ。単純なレスポンスキャッシュとSEGLAMの推論キャッシュは根本的に異なる。

**レスポンスキャッシュ**は「Q：韓国の首都は？ -> A：ソウル」を格納する。質問が完全一致したときだけヒットする。「大韓民国の首都は？」と聞けばミスする。これは辞書であって知能ではない。

**SEGLAMの推論キャッシュ**は「この種の質問に対して、この手続きで答えを組み立てる」というコードを格納する。具体的な値ではなく推論パスそのものを結晶化する。したがって入力が変わっても、同じ種類の質問ならヒットする。これは理解だ。これは成長だ。

たとえるなら：レスポンスキャッシュは九九を暗記する。推論キャッシュは掛け算の仕方を学ぶ。

---

## 時間の経過で何が起こるか

この設計の最も強力な特性は、**時間が味方する**ことだ。

- **1日目：**キャッシュは空。ほぼすべてのリクエストがキャッシュミス。LLMがフル稼働する。遅くて高コスト。
- **30日目：**かなりの割合のルーチン推論パターンがキャッシュされている。LLM呼び出しが減少。
- **365日目：**ほとんどのリクエストがキャッシュヒット。LLMは本当に新しいタイプの問題にのみ呼び出される。システムは高速、低コスト、予測可能。
- **それ以降：**キャッシュ自体がそのドメインの「結晶化された知能」になる。ポータブルで、検証可能で、蓄積可能な知的資産。

LLMへの依存は時間とともに減少する。
システム効率は時間とともに増加する。
この曲線は決して逆転しない。

---

## 推論保存の原則

このアプローチの最も根本的な原則は：

> 「AIの推論過程は捨ててはならない——記録しなければならない。」

推論キャッシュはこの哲学の最も直接的な実装だ。

LLMが一度行った推論は構造化表現に結晶化されて格納される。捨てられない。再利用される。検証される。改善される。蓄積される。

そしてキャッシュされたコードは明確な構造化言語で記述されているため：

- 特定の手続きがなぜ作成されたかを**追跡**でき、
- 手続きが間違いとわかったときに**修正**でき、
- より良い手続きが見つかったときに**置換**できる。

ブラックボックスの中で呼び出すたびに蒸発する推論ではなく、
ホワイトボックスの上に蓄積される知能。それが追求すべきAIのビジョンだ。

---

## まとめ

| 従来のLLM | SEGLAM |
|-----------|--------|
| リクエストのたびにゼロから推論 | キャッシュヒット時にキャッシュコードを実行 |
| 推論結果が蒸発 | 推論がコードに結晶化し蓄積 |
| コストが使用量に比例して増大 | コストが時間とともに減少 |
| LLM = ランタイムエンジン | LLM = コンパイラ |
| ブラックボックス推論 | 検証・修正・置換可能なコード |

毎回のリクエストでLLMを呼び出すのは、隣の家に行くのに飛行機に乗るようなものだ。
一度道を舗装すれば、以後は歩いて行ける。

SEGLAMは道を舗装するシステムだ。
