---
title: "なぜ埋め込みベクトルでは駄目なのか"
weight: 11
date: 2026-02-26T12:00:18+09:00
lastmod: 2026-02-26T12:00:18+09:00
tags: ["埋め込み", "ベクトル", "ホワイトボックス"]
summary: "埋め込みベクトルを並べ替えるとモデルが壊れる。壊さずに並べ替えるにはモデルを作り直すしかない。ブラックボックスの内部を透明にするのではなく、外部に透明な層が必要だ。"
author: "Junwoo Park"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## ベクトルは演算には優れているが、解釈は不可能だ。ブラックボックスの内部を透明にすることはできない。

---

## 埋め込みベクトルは驚くべき技術だ

「王 - 男 + 女 = 女王」

word2vecがこれを示したとき、世界は驚嘆した。
単語を数百次元のベクトルで表現すれば、
意味の関係がベクトルの演算として現れる。

埋め込みベクトルはLLMの基盤だ。
トランスフォーマーのすべてはベクトル演算である。
トークンがベクトルになり、
アテンションがベクトル間の類似度を計算し、
出力がベクトルからトークンに変換される。

似た意味は近いベクトル。
異なる意味は遠いベクトル。
検索はベクトル類似度の計算。
分類はベクトル空間における境界の設定。

埋め込みベクトルがなければ、今日のAIは存在しない。

ならば、埋め込みベクトルで知識を表現すればいいのではないか。
ベクトルを直接整列し、構造化し、解釈可能にすれば。

できない。
これを最も確実に知る方法は、試してみることだ。

---

## AILEV：試みた

GEULプロジェクトは当初、AILEVという名前で始まった。

AI Language Embedding Vector。

名前そのものが目的を語っている。
埋め込みベクトルを直接操作するAI言語。

構想はこうだった。

512次元のベクトルで意味を表現する。
ベクトルの区間に役割を割り当てる。
先頭128次元はエンティティ、次の128次元は関係、次の128次元は属性、残りはメタデータ。
RGBAが色を4つのチャネルに分解するように、意味を次元の区間に分解する。

BERTを学習させて自然言語をこの構造化ベクトルに変換する。
「ソウルは韓国の首都」が入力されると、
エンティティ区間にソウルのベクトル、関係区間に首都のベクトル、属性区間に韓国のベクトルが出力される。

ベクトルだから演算が可能だ。
類似度検索が可能だ。
次元を削減すれば優雅な劣化も実現する。
512次元から256次元に減らせば精度は落ちるが、核心的な意味は保持される。

優雅だった。理論上は。

---

## なぜ失敗するのか

### ベクトルを恣意的に並べ替えるとモデルが壊れる

LLMの埋め込みベクトルは学習の結果だ。

数十億のテキストを読んで、
モデルが自ら最適化した内部表現だ。
各次元が何を意味するかはモデルが決めたことであり、
人間が決めたことではない。

「先頭128次元はエンティティ」と定めるとどうなるか。

モデルが学習したベクトル空間では、
エンティティの情報は先頭128次元にはない。
768次元全体に分散している。
関係情報も、属性情報も、時制情報もすべて混在している。

これは設計のミスではなく、学習の本質だ。
誤差逆伝播（backpropagation）は
タスク遂行に最適なベクトル配置を見つける。
解釈可能な配置を見つけるのではない。
最適と解釈可能は同じではない。

ベクトルを強制的に再配置すると——「エンティティはここ、関係はあそこ」——
モデルが学習した統計的関係が壊れる。
性能が低下する。

### 壊さずに並べ替えることはモデルを作り直すことだ

では最初から「先頭128次元はエンティティ」という制約をかけて学習させればよいのではないか。

できる。理論上は。
しかしこれは埋め込みベクトルを整列させることではなく、
新しいモデルアーキテクチャを設計することだ。

学習データが必要だ。数十億トークン。
学習インフラが必要だ。数千GPU。
学習時間が必要だ。数か月。
そしてそのモデルが既存のLLMと同等に機能する保証はない。

規模が大きすぎる。

「ベクトルを整列して解釈可能にする」という問題が、
「LLMをゼロから作り直す」という問題に変わった。
これは問題の解決ではなく、問題の拡大だ。

### 解釈は不可能だ

仮に構造化ベクトルが作れたとしよう。
512次元のベクトルがある。
先頭128次元がエンティティだとしよう。

エンティティ区間の値は `[0.23, -0.47, 0.81, 0.12, ...]` だ。

これが「サムスン電子」なのか「現代自動車」なのか、
どうやって判別するのか。

最も近いベクトルを探さなければならない。
ベクトルデータベースで類似度を計算しなければならない。
そして「おそらくサムスン電子だろう」という確率的な答えを得る。

「おそらく。」

ベクトルは本質的に連続だ。
サムスン電子とSKハイニックスのベクトルの間には、
無限の中間ベクトルが存在する。
その中間ベクトルが何を意味するか、誰にもわからない。

これは技術的限界ではなく、数学的本質だ。
連続空間で離散的な意味を表現すると、
境界が曖昧になる。
曖昧さは[自然言語の問題](/ja/why/natural-language-hallucination/)だった。
ベクトルに変えたのに、曖昧さが再び現れた。

形が変わっただけだ。
自然言語では言葉の曖昧さ。
ベクトルでは座標の曖昧さ。

---

## ホワイトボックス原則

ここで根本的な設計原則の問題が露呈する。

埋め込みベクトルはブラックボックスだ。
768次元の実数ベクトルを見ても、
どんな情報がどこにどう格納されているか、
人間にはわからない。
モデルも説明できない。

これは不便な特性ではなく、存在論的属性だ。
ベクトルが機能する理由がまさにこれだからだ。
人間が設計しなかった方法で情報を配置するからこそ、
人間が設計したものより優れた性能を発揮する。
解釈不可能性はバグではなくフィーチャーだ。

しかし、AIのコンテキストとして使われる知識は正反対の要求を受ける。

出典を知る必要がある。
時点を知る必要がある。
確信度を知る必要がある。
何についての記述かを知る必要がある。
二つの記述が同じ対象についてのものかを知る必要がある。

すべて「知る必要がある」だ。すべて解釈可能性の要求だ。

ブラックボックスであるベクトルで、ホワイトボックスの要求を満たすことは
矛盾だ。

---

## 転換の論理

AILEVからGEULへの転換は撤退ではなかった。
問題の再定義だった。

**元の問題：** LLMはブラックボックスだ。内部を透明にしよう。
→ 埋め込みベクトルを整列して解釈可能にしよう。
→ ベクトルに手を加えるとモデルが壊れる。
→ 壊さないようにするにはモデルを作り直すしかない。
→ 行き止まり。

**再定義された問題：** ブラックボックスの内部を透明にする必要はない。外部に透明な層を作ろう。
→ LLMの内部には手を加えない。
→ LLMの外に、解釈可能な表現体系を作る。
→ LLMはその表現体系を読み書きできる。トークンだから。
→ 人工言語。

ベクトルではなく言語。
連続的ではなく離散的。
解釈不可能ではなく、解釈が唯一の目的。
モデルの内部ではなく、モデルの外部。

AILEVの「Embedding Vector」が外れ、
GEULの「글（文字）」になった理由がこれだ。

---

## ベクトルは演算に、言語は表現に

これは埋め込みベクトルを否定するものではない。

ベクトルは演算に最適化されている。
類似度検索、クラスタリング、分類、検索。
ベクトルの仕事を言語が代替することはできない。

言語は表現に最適化されている。
エンティティの同一性、関係の記述、メタデータの内包、解釈可能性。
言語の仕事をベクトルが代替することはできない。

両者は異なる層のツールだ。

LLMの内部ではベクトルが動く。ブラックボックスだ。そうあるべきだ。
LLMの外部では言語が動く。ホワイトボックスだ。そうあるべきだ。

問題はこの二つの層を混同したことから始まった。
ベクトルに言語の仕事をさせようとした。
ブラックボックスにホワイトボックスの役割を担わせようとした。

それぞれに居場所がある。

---

## まとめ

埋め込みベクトルはLLMの基盤であり、驚くべき技術だ。
しかし知識表現の手段としては根本的な限界がある。

GEULはAILEV（AI Language Embedding Vector）として始まった。
ベクトルを直接整列して解釈可能にしようとした。
失敗した。二つの理由で。

ベクトルを恣意的に整列するとモデルが学習した関係が壊れる。
壊さずに整列するにはモデルをゼロから作り直す必要がある。規模が大きすぎる。

そして仮に成功しても、ベクトルは解釈できない。
連続空間では離散的な意味の境界は曖昧だ。
ブラックボックスにホワイトボックスの役割は担えない。

転換の論理：
ブラックボックスの内部を透明にしようとした。
内部に手を加えると壊れる。
内部には手を加えず、外部に透明な層を別途作る。
ベクトルではなく言語。モデルの内部ではなく外部。

ベクトルは演算に、言語は表現に。
それぞれに居場所がある。
