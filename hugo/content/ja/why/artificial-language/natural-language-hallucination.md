---
title: "なぜ自然言語は幻覚を生むのか?"
weight: 8
date: 2026-02-26T12:00:16+09:00
lastmod: 2026-02-26T12:00:16+09:00
tags: ["自然言語", "幻覚", "曖昧性"]
summary: "幻覚はLLMのバグではない——自然言語の4つの構造的欠陥（曖昧性・出所不在・確信度不在・時点不在）から生じる必然だ。モデルを大きくしても解消できない。"
author: "박준우"
authorLink: "https://parkjunwoo.com/1/about"
image: "/images/og-default.webp"
---

## 幻覚はバグではない。自然言語を使う限り、それは構造的必然だ。

---

## 自然言語の奇跡

10万年前、話し言葉が生まれた。霊長類が毛づくろいで維持できた社会的関係は約150個体が限界だった。言語がその天井を打ち破った。一人が同時に多くの人に話しかけられるようになると、新しいスケールの社会——部族——が可能になった。

1万年前、農業が食糧の余剰を生み、人々は一箇所に集まって都市を形成した。5千年前、メソポタミアの誰かが湿った粘土板にくさび形の記号を押した。穀物の在庫を記録するためだった。文字の誕生。話し言葉は消えるが、記録は残る。記録が残るようになると、官僚制度が可能になり、法が可能になり、国家が可能になった。

話し言葉は部族を創った。文字は国家を創った。

自然言語は人類が創った最も偉大な技術だ。火の発見でもなく、車輪の発明でもなく、半導体の発明でもない。それらすべてを可能にしたのが自然言語だ。自然言語があったから知識が伝達でき、協力が生まれ、死者の思考を生者が受け継ぐことができた。数万年にわたり、自然言語はあらゆる人類文明の媒体だった。

そして今、その偉大な自然言語がAI時代のボトルネックになっている。

---

## 「幻覚」という誤解

AIが偽りを述べるとき、私たちはそれを「幻覚」と呼ぶ。

この名称にはいくつかの含意がある。
幻覚は異常であるという含意。
修正可能であるという含意。
より良いモデルが解決するという含意。

これは誤解だ。

幻覚はLLMのバグではない。
幻覚は、自然言語がAIの推論言語として使われる限り
避けられない構造的必然だ。

モデルをいくら大きくしても、
データをいくら増やしても、
RLHFをいくら精緻にしても、
入力が自然言語で出力も自然言語である限り、
幻覚は消えない。

その理由を説明する。

---

## 自然言語の4つの構造的欠陥

自然言語は人間同士のコミュニケーションのために進化した。
その過程で獲得した4つの特性が、
AI推論においては致命的欠陥になる。

---

### 欠陥1：曖昧性

"He went to the bank."

"bank"は金融機関か河岸か？
"he"は誰か？
いつ行ったのか？

人間はコンテキストで解決する。
会話の流れ、話者の表情、共有された背景知識。

AIにはテキストしかない。
テキストだけでは曖昧性を完全には解消できない。
解消できなければ、AIは推測する。
推測は時として誤る。
誤った推測が自信を持って出力されると、それが幻覚だ。

---

### 欠陥2：出所の不在

「李舜臣はわずか12隻で133隻を撃破した。」

この文には出所がない。

誰がこの主張をしたのか？
どの歴史記録がそれを裏付けるのか？
この数字に学術的な異論はないのか？

自然言語にはメタデータのための構造的な場所がない。
出所を含めるには文を長くする必要があり、
長くすると要点がぼやける。
だからほとんどの自然言語の文では出所が省略される。この問題は[なぜ事実ではなく主張なのか？](/ja/why/claims-not-facts/)でさらに掘り下げている。

LLMは数十億のそのような文で訓練される。
出所が省略された主張が混ざり合い、
一つの巨大な統計的スープになる。

そのスープの中で「12」という数字の根拠を辿ることは
原理的に不可能だ。
根拠を辿れないからこそ、根拠のない数字も捏造されうる。
それが幻覚だ。

---

### 欠陥3：確信度の不在

「地球は丸い。」
「ダークエネルギーは宇宙の68%を占める。」
「明日は雨が降る。」

これら3つの文の確信度はまったく異なる。

最初は圧倒的な合意だ。
2番目は現在の最良推定だが、理論は変わりうる。
3番目は確率的予測だ。

しかし自然言語では、3つとも同一の文法構造を持つ。
主語 + 述語。平叙文。句点。

自然言語は「これがどの程度確かか」を構造的に表現できない。
「おそらく」「ほぼ確実に」「かもしれない」のような副詞的手段はあるが、
それらはオプションであり、不正確であり、たいてい省略される。

LLMはすべての文を同一の確信度で学習する。
「地球は丸い」と「ダークエネルギーは68%」の確信度の違いを
モデル内部で区別する方法がない。

だから推定を事実として述べ、
仮説を定説として述べ、
不確かなことを確かに述べる。
それが幻覚だ。

---

### 欠陥4：時間的コンテキストの不在

「テスラのCEOはイーロン・マスクだ。」

いつの時点で？

2024年にはこれは正しい。
2030年にはわからない。
書かれた時点が明記されていなければ、
この文の有効期間を確定できない。

ほとんどの自然言語の文は時間的コンテキストを省略する。
「現在形」は「今この瞬間」を意味することも
「一般的に」を意味することもある。

LLMは2020年の記事と2024年の記事を同じデータとして学習する。
時間情報が構造的に保存されないため、
過去の事実を現在のことのように述べたり、
異なる時期の情報を混同したりする。
それが幻覚だ。

---

## 4つの欠陥の合流

これら4つの欠陥が合流すると、幻覚は爆発的にエスカレートする。

一つのLLM出力を分析してみよう。

> 「李舜臣は12隻で330隻の日本船を殲滅し、
> その後露梁海戦で戦死した。遺言は『我が死を告げるな』であった。」

この文には：

**曖昧性：**「殲滅」の正確な意味は？撃沈？追散？部分損傷？

**出所の不在：**12と330の根拠は何か？異なる歴史記録は異なる数字を挙げている——どれに従ったのか？

**確信度の不在：**「我が死を告げるな」は歴史的に確認された遺言か、後世の口伝か？両者の確信度は異なるのに、同じ平叙文で並べられている。

**時間的コンテキストの不在：**この情報はどの時点の学術的合意を反映しているのか？

LLMはこれらすべての曖昧性を「最も妥当なトークン列」で埋める。
妥当性は正確性ではない。
両者の間の隙間が幻覚だ。

---

## なぜ大きなモデルでは解決できないのか

「GPT-5が出れば幻覚は減るのでは？」

減る。しかし消えない。

大きなモデルはより多くのデータからより精密なパターンを学習する。
だから「妥当性」の精度が上がる。

しかし根本的問題は変わらない。

入力が自然言語である限り、曖昧性は残る。
訓練データが自然言語である限り、出所は失われたままだ。
出力が自然言語である限り、確信度は表現されない。
構造に時間情報がない限り、時間は混乱したままだ。

モデルを100倍に拡大しても、
自然言語の構造的欠陥は100倍には成長しない——
しかしゼロにもならない。

これは解像度の問題ではない。媒体の問題だ。

白黒写真の解像度をいくら上げても、色は現れない。
自然言語の精度をいくら上げても、
出所、確信度、時間的コンテキストは構造に現れない。

色が欲しければ、カラーフィルムが必要だ。
幻覚を消したければ、別の言語が必要だ。

---

## 構造的解決の条件

これら4つの欠陥を解決するには、言語自体の構造が異ならなければならない。

**曖昧性 --> 明示的構造化。**
"He went to the bank"が構造化言語に変換されるとき、
"he"は特定のエンティティSIDXに解決され、
"bank"は金融機関または河岸のSIDXに解決される。
解決できなければ、「未解決」が明示的に宣言される。
曖昧性を解消するか、曖昧であるという事実を記録するか。

**出所の不在 --> 出所の埋め込み。**
すべての語りに出所エンティティが構造的に含まれる。
「誰がこの主張をしたか」が語りの一部だ。
オプションではない。フィールドが空であれば、空と記される。

**確信度の不在 --> 確信度の埋め込み。**
すべての動詞エッジに確信度フィールドがある。
「確実」「推定」「仮説」が
動詞修飾子として構造的に指定される。

**時間的コンテキストの不在 --> 時間的コンテキストの埋め込み。**
すべての語りに時間的コンテキストが含まれる。
「この語りはいつの時点か」が常に指定される。

自然言語で省略されるものが、
構造化言語では構造の一部として存在する。

省略が不可能になれば、幻覚の余地は縮小する。この原理は[なぜ明確化が必要なのか](/ja/why/clarification/)で説明している。
根拠なしに語れなくなれば、根拠のない発言は生まれない。

---

## 幻覚の終わりは言語の置換にある

現在の幻覚軽減アプローチを見てみよう。

**RAG（検索拡張生成）：**外部文書を検索しコンテキストとして提供する。有効だが、検索された文書も自然言語であるため、曖昧性、出所不在、確信度不在の問題がそのまま付いてくる。この限界については[なぜRAGだけでは不十分なのか](/ja/why/rag-not-enough/)で詳しく論じている。

**RLHF：**不確かなときに「わかりません」と答えるようモデルを訓練する。幻覚の頻度を減らすが、自然言語に確信度構造が欠如しているという根本的問題は解決しない。

**Chain-of-Thought：**推論過程を自然言語で記録する。方向は正しいが、記録の媒体が自然言語であるため、同じ欠陥を引き継ぐ。

これらのアプローチはすべて、自然言語の枠組み内で幻覚を軽減しようとするものだ。
有効だ。しかし根本的ではない。

根本的な解決策は、AIの内部から自然言語を取り除くことだ。

ユーザーとのインターフェースは自然言語のままだ。
人間は引き続き自然言語で話し、自然言語で回答を受け取る。

しかしAIが内部で推論し、記録し、検証する言語は
自然言語以外のものでなければならない。

出所が構造の中にある言語。
確信度が構造の中にある言語。
時間的コンテキストが構造の中にある言語。
曖昧性が明示的に扱われる言語。

話し言葉は部族を創った。
文字は国家を創った。
第三の言語は何を創るか？

幻覚の終わりは、より大きなモデルにではなく、
より良い言語にある。

---

## まとめ

幻覚は自然言語の4つの構造的欠陥から生まれる。

1. **曖昧性：**コンテキストなしには解消できない。AIは推測し、推測は誤る。
2. **出所の不在：**主張の根拠が失われる。根拠のない組み合わせが捏造される。
3. **確信度の不在：**事実と推定が同一の文法で表現される。AIは区別できない。
4. **時間的コンテキストの不在：**異なる時期の情報が混同される。

大きなモデルは幻覚を減らすが、消すことはできない。
媒体を変えなければ、構造的欠陥は残る。

白黒フィルムの解像度をいくら上げても、色は現れない。
色が欲しければ、フィルムを変えなければならない。
