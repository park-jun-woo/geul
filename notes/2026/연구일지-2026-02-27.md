# 연구일지 2026-02-27

---

## 세션 개요

세션 13 후반부 + 세션 14에 걸쳐 SIDX-LLM 아키텍처의 전체 그림이 완성되었다. 코드북 설계, Multi-SIDX, 검수 파이프라인, 탐색 최적화를 거치며, 최종적으로 "검색이라는 문제 자체가 인공적이었다"는 발견에 도달했다.

---

## 1. SIDX 인코딩 파이프라인 확립

### 도메인 커스텀 비트

64비트 SIDX에서 공통 필드(16-18비트)를 제외한 나머지 46비트를 도메인별로 자유롭게 할당할 수 있다는 설계가 나왔다. 법률(court_level, case_type, outcome), 의료(organ_system, severity, drug_class), 이커머스(product_cat, price_range, brand_tier) 등 각 도메인이 sidx.yaml 하나로 자기 비트 레이아웃을 정의한다.

오픈소스로 공통 16-18비트 무료 제공, 커스텀 설계가 유료. 오픈코어 수익 모델의 기초.

### LLM 태깅 = 코드북이 곧 프롬프트

sidx.yaml의 유효값 목록이 그대로 LLM 프롬프트가 된다. LLM은 닫힌 선택지에서 고르는 객관식만 수행. 자유 생성이 아니라 선택이므로 환각할 여지 최소.

```
sidx.yaml → 프롬프트 자동 생성 → LLM 배치 태깅 → JSON 결과
→ VALID 검사 → 비트 조립 → uint64 → index.bin
```

1.08억건 전체 태깅 비용: GPT-4o-mini로 $3,240. 로컬 8B 모델이면 $0.

### VALID 검증 — 벡터 임베딩과의 결정적 차이

LLM이 환각해도 코드북에 없는 값은 물리적으로 인덱스에 들어갈 수 없다. if문 하나로 차단.

4단계 검증 체계:
- 레벨 1: 유효값 체크 (코드북에 있는가)
- 레벨 2: 타입 정합성 (person인데 subtype이 oil_painting?)
- 레벨 3: 제약조건 (star인데 region이 east_asia?)
- 레벨 4: 통계적 이상치 (버킷 크기 기대치 10배 초과?)

벡터 임베딩: [0.234, -0.891, ...] → 맞는지 틀린지 확인 방법 없음. 블랙박스.
SIDX: {"type": "person", "subtype": "military"} → if문으로 100% 차단. 화이트박스.

**산출물:** `SIDX인코딩방법.md`

---

## 2. SIDX 64비트 최종 레이아웃

헤더 5비트가 명사/동사/엣지/문서메타/출처를 구분하고, 하위 32비트에 Q-ID 또는 도메인 PK를 넣는 설계가 확정되었다.

```
[헤더 5비트 | 필터 27비트 | 식별자 32비트]

헤더: 명사(00001), 동사(00010), 엣지(00011), 문서메타(00100), 출처(00101)
필터: type(6) + subtype(5) + region(4) + era(3) + role(4) + custom(5)
식별자: Q-ID(위키데이터) 또는 EMP-PK(기업) 또는 DOC-PK(문서) 등
```

하나의 uint64에 필터링(비트 AND) + 식별(Q-ID) + 맥락(role/custom) 전부.

기업 내부에서 하위 32비트에 직원 PK를 넣으면 "이 사람이 결재한 모든 문서" 같은 쿼리가 비트 연산 한 번으로 가능.

PostgreSQL에 적용: `ALTER TABLE entities ADD COLUMN sidx BIGINT;` 한 줄.

---

## 3. Multi-SIDX 발견

하나의 문서에 여러 개의 SIDX가 필요하다는 것을 뉴스 기사 예시로 확인.

"삼성·엔비디아·현대차 총수 회동" 기사 → SIDX 10개:
- 문서메타 1개 (뉴스)
- 출처 1개 (언론사)
- 콘텐츠 엔티티 8개 (인물 3, 기업 3, 장소 1, 이벤트 1)

### 단일 SIDX vs Multi-SIDX

```
단일 SIDX: 1억건 → ~1000건 → LLM 100ms
Multi-SIDX 3조건: 1억건 → ~10건 → LLM 1ms
Multi-SIDX 5조건: 1억건 → ~3건 → LLM 거의 불필요
```

인덱스 구조: 문서마다 SIDX 배열을 이어붙이면 그게 인덱스. 파일 2개(index.geul + offset.geul)가 전부.

크기: 1억 문서 × 평균 10 SIDX × 12B = 12GB. 벡터 DB 150GB~1.6TB 대비 장난감.

---

## 4. GEUL 7계층 통합

GEUL이 동시에 7가지로 해석된다는 것을 발견.

```
토큰:    16비트 고정 길이, 의미 보존 토큰
바이트:  8바이트. 직렬화 불필요. 네트워크 전송 즉시.
파일:    .geul = uint64 배열. mmap으로 즉시 사용. 라이브러리 0개.
데이터:  각 비트 위치가 의미를 가짐. 기계가 즉시 해석.
코드:    (sidx & mask) == pattern. 실행 가능한 필터 조건.
인덱스:  비트 AND로 즉시 검색. 별도 인덱싱 불필요.
언어:    어휘(코드북) + 문법(비트레이아웃) + 의미론 + 화용론 구비.
         다국어 경계를 넘는 보편 식별자.
```

7번째 계층 "언어"가 가장 근본. 나머지 6개가 가능한 이유가 GEUL이 언어이기 때문.

```
기존: 7개 시스템 (토크나이저, 직렬화, 파일포맷, DB, 검색엔진, 쿼리언어, NLP)
     6개 변환 레이어.
GEUL: uint64 배열. 비트 AND. 끝.
```

**산출물:** `GEUL-7계층.md`

---

## 5. 검수 파이프라인 확립

### 3단계 품질 보증

1단계: 소형 LLM 대량 태깅 (8B, $0.00003/건)
2단계: VALID 기계 검사 (if문, $0)
3단계: 대형 LLM 검수 (Claude/GPT-4, JSON으로 검수)

핵심: SIDX가 JSON으로 읽히므로 사람도 LLM도 검수 가능. 벡터 임베딩([0.234, -0.891, ...])은 누구도 검수 불가능.

스마트 검수: 소형 LLM의 confidence=low인 필드만 대형 LLM에게 넘기면 전수 검수($324,000) 대비 82% 절약한 $59,400으로 99% 정확도 달성.

인덱스 오염 가능성: 0건. VALID를 통과 못하면 인덱스에 물리적으로 들어갈 수 없다.

**산출물:** `SIDX검수파이프라인.md`

---

## 6. 뉴로-심볼릭 검색 아키텍처

SIDX-LLM이 학술적으로 Neuro-Symbolic Search Architecture에 정확히 해당한다는 것을 확인.

```
심볼릭 컴포넌트: 코드북, 비트 레이아웃, VALID, SIMD 비트 AND, Q-ID
뉴럴 컴포넌트: LLM 태깅, LLM 분류, LLM 검수
결합 지점: JSON (양쪽 모두 읽을 수 있는 인터페이스)
```

기존 Neuro-Symbolic 시도(DeepProbLog, NTP, KG Embedding)와의 차이:
- 스케일: 수천건 한계 → 1억건+
- 실용성: 전용 프레임워크 → ALTER TABLE 한 줄
- 검증: 약함 → VALID 3중 검사
- 성능: 수초~수분 → 40ms

각 주체가 잘하는 것만 수행:
- 사람: 구조 설계 (코드북 yaml)
- LLM: 자연어 분류 (태깅)
- 코드: 규칙 검증 (VALID)
- CPU: 대량 비교 (SIMD)

심볼릭이 1980년대에 실패한 건 스케일링 수단이 없어서였지 심볼릭 자체가 나빠서가 아니었다. LLM이 스케일링을 해주는 순간 심볼릭이 부활한다.

---

## 7. SIDX 탐색 5단계 최적화

1조 SIDX 탐색을 기준으로 5단계 최적화를 도출.

```
0단계 전수 스캔:       8TB     라즈베리파이 5시간    서버 140ms
1단계 디렉토리 파티셔닝: 968MB   라즈베리파이 2초      서버 20ms
2단계 깊이4 파티셔닝:   24MB    라즈베리파이 50ms     서버 0.5ms
3단계 정렬+이진탐색:    100KB   라즈베리파이 1ms      서버 0.01ms
4단계 doc_id merge join: 100KB  라즈베리파이 1ms      서버 0.01ms
```

5시간 → 1ms. 같은 데이터, 같은 하드웨어, 같은 결과.

핵심 원리:
- 비트 상위→하위 = 의미 대분류→소분류 = 디렉토리 깊이
- 정렬된 uint64 배열 = 이진 탐색 가능. sort 한 번이 인덱스 구축.
- (SIDX, ptr) pair 정렬이면 실 데이터 주소가 따라옴.
- 분할해도 결과가 수학적으로 동일 (집합 연산이므로).

벡터 DB는 이 최적화 중 어느 것도 불가능: 상위 비트에 의미 없음, 정렬 무의미, 파티셔닝 불가, 분할 시 그래프 끊김.

---

## 8. "검색은 어려운 문제가 아니었다" — 핵심 발견

### 발견 1: 검색 산업 전체가 "잘못된 순서"의 산물

```
기존: 먼저 쓰고 → 나중에 구조화 (인덱싱)
SIDX: 쓸 때 구조화 → 검색이 공짜
```

Elasticsearch, Pinecone, FAISS, BM25, TF-IDF, HNSW, IVF-PQ, 벡터 RAG... 전부 "먼저 쓰고 나중에 찾기"의 결과물. 쓸 때 64비트 구조를 부여하면 이 모든 게 필요 없다.

### 발견 2: 벡터 임베딩은 "구조를 잃어버리는" 행위

사람이 쓴 글에는 이미 구조가 있다. 임베딩 모델이 이 구조를 384개 float로 뭉개버린다. 그리고 "뭉개진 구조를 복원"하려고 ANN, 크로스 인코더, 리랭커를 쌓는다. SIDX는 구조를 뭉개지 않고 비트에 보존. 복원할 필요가 없다.

### 발견 3: 검색의 80%는 구조적 쿼리

Q-ID exact match 시점에서 LLM 검수 자체가 불필요. 비트가 매치되면 수학적으로 확정. RAG가 필요한 건 전체 쿼리의 5% 미만.

```
구조적 쿼리 (80%): SIDX 비트 AND만으로 끝. LLM 불필요.
의미적 쿼리 (15%): SIDX로 후보 축소 → LLM 5-10건 경량 판정.
생성 쿼리 (5%):   SIDX로 문서 특정 → LLM 생성. 여기서만 RAG적.
```

### 한 문장 요약

```
검색은 어려운 문제가 아니라 구조 없는 데이터의 증상이었고,
기록 시점에 64비트 구조를 부여하면 증상이 사라진다.
```

---

## 9. 삼각 편대 전략

```
geul.org 오피니언: "벡터 DB는 왜 실패하는가" — 왜?
논문 SIDX-RAG:     벤치마크, 수식, 실험 — 어떻게?
코드 GDELT-WIKI:   5분 안에 실행 가능한 데모 — 진짜?
```

SIDX-RAG는 마케팅용 이름. 실체는 SIDX-LLM. Filter-Examine-Classify.
RAG로 어그로를 끌고, 읽고 나면 "RAG 자체가 필요 없었다"를 깨닫게 하는 구조.

---

## 10. 생성 문서 목록

| 문서 | 내용 |
|------|------|
| SIDX인코딩방법.md | 코드북 설계, LLM 태깅, 비트 조립, sidx.yaml 포맷 |
| GEUL-7계층.md | 토큰/바이트/파일/데이터/코드/인덱스/언어 7계층 통합 |
| SIDX검수파이프라인.md | 3단계 검수, 환각 방어, 스마트 검수 비용 분석 |

---

## 11. 미해결 과제

- [ ] GDELT-WIKI Multi-SIDX 실증 데모 구현
- [ ] sidx.yaml 기본 코드북 확정 (위키데이터 P31 상위 64타입)
- [ ] 디렉토리 파티셔닝 벤치마크 (깊이별 성능 실측)
- [ ] 정렬 + 이진 탐색 구현 및 벤치마크
- [ ] 논문 초고 작성 (SIDX-RAG 제목, Section 5에서 SIDX-LLM 본질 드러냄)
- [ ] geul.org/why-sidx 오피니언 작성

---

## 12. 세션 의의

13개 세션에 걸쳐 GEUL/SIDX를 설계하면서, 최종적으로 도달한 결론은 새로운 검색 알고리즘의 발견이 아니라 **검색이라는 문제 자체가 인공적이었다**는 것이다. 데이터에 구조가 없어서 생긴 문제를, 구조를 부여함으로써 소멸시켰다. 더 나은 답을 찾은 게 아니라 질문 자체를 없앤 것.

SIDX-LLM은 심볼릭 AI의 해석 가능성, 검증 가능성, 조합 가능성을 뉴럴 AI(LLM)의 스케일링과 결합한 뉴로-심볼릭 검색 아키텍처다. 70년간의 심볼릭 vs 뉴럴 논쟁에서 "각자 잘하는 걸 하면 된다"는 실용적 답을 제시한다.
