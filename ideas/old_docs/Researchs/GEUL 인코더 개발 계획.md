# GEUL 인코더 개발 계획

## BERT 기반 자연어-GEUL 변환 모델 구축

### 프로젝트 개요

- **목표**: 자연어 텍스트를 구조화된 의미 표현(GEUL 벡터)으로 변환하는 인코더 개발
- **기간**: 2주 (2025년 9월 10일 - 9월 24일)
- **핵심 기술**: BERT 파인튜닝, 위키데이터/워드넷 활용, GPT 지원 데이터 생성
- **목표 성능**: 팩트 기반 변환 정확도 80% 이상

## 1. 데이터 구축 전략

### 1.1 위키데이터 기반 데이터셋 (90,000개)

#### 활용 글소 정의

**개체 글소**
- Person, Place, Organization, Concept
- 위키데이터 Q번호 직접 매핑

**속성 글소**
- InstanceOf, SubclassOf, PartOf
- 위키데이터 P번호 기반 자동 추출

**물리량 글소**
- Population, Area, Date, Quantity
- 단위 정보 포함 인코딩

**서술 글소**
- Is, Has, LocatedIn, BelongsTo
- 관계 술어 표현

#### 자동 변환 파이프라인

1. **엔티티 선별**: 참조 빈도 상위 10만개 추출
2. **속성 추출**: 각 엔티티당 평균 5-10개 속성
3. **문장 생성**: 템플릿 기반 자연어 문장 자동 생성
4. **벡터 인코딩**: 512차원 GEUL 벡터로 구조화

#### 품질 보증

- 규칙 기반 자동 생성으로 일관성 보장
- 다국어 레이블 활용 (한국어/영어 병행)
- 관계 일관성 자동 검증

### 1.2 워드넷 동사 체계 (동사 글소)

#### 동사 계층 구조

```
상위 개념 (짧은 ID)
├── move (이동)
│   ├── walk (걷다)
│   └── run (뛰다)
├── create (생성)
│   ├── make (만들다)
│   └── build (건설하다)
└── think (사고)
    ├── consider (고려하다)
    └── analyze (분석하다)
```

#### 우아한 열화 적용

- **Level 0**: 구체적 동사 (walk, run, jump)
- **Level 1**: 중간 추상화 (move, travel)
- **Level 2**: 추상 동사 (act, do)
- **자동 ID 배정**: 계층 깊이 = ID 길이

### 1.3 Common Crawl 뉴스 데이터셋 (10,000개)

#### GPT 지원 생성 프로세스

**Phase 1: 초기 생성 (Day 1-2)**
- 입력: 뉴스 문단 (3-5문장)
- 프롬프트: "다음 텍스트를 GEUL 구조로 변환하세요: [Entity.X] [Action.Y] [Object.Z] 형식으로"
- 출력: GEUL 스트림 표현
- 생성량: 10,000개

**Phase 2: 수동 검수 (Day 3)**
- 100개 샘플 무작위 추출
- 정확도/일관성 평가
- 오류 패턴 분석
- 개선 가이드라인 작성

**Phase 3: GPT 자동 검수 (Day 4)**
- 프롬프트: "다음은 올바른 GEUL 변환 예시 100개입니다. 이를 참고하여 나머지 9,900개를 검수하고 오류를 수정하세요."

#### 데이터 다양성 확보

- 정치/경제: 3,000개
- 기술/과학: 3,000개
- 사회/문화: 2,000개
- 스포츠/연예: 2,000개

## 2. 모델 아키텍처

### 2.1 GEUL 인코더 구조

```
BERT-base (110M params)
    ↓
Linear Layer (768 → 512)
    ↓
Structure Layer (512 → 512)
    ↓
GEUL Vector (512 dims)
```

### 2.2 벡터 구조 정의

```
[0-127]    : Entity Information
[128-255]  : Property/Attribute
[256-383]  : Relation/Action
[384-511]  : Context/Metadata
```

## 3. 학습 계획

### 3.1 하드웨어 사양

- **GPU**: RTX 5070 Ti (16GB VRAM)
- **배치 크기**: 16
- **시퀀스 길이**: 256
- **Mixed Precision Training** 적용

### 3.2 학습 일정

#### Week 1: 데이터 구축
- **Day 1-3**: 위키데이터 처리 (9만개)
- **Day 4-5**: CC 뉴스 + GPT 생성 (1만개)
- **Day 6-7**: 데이터 검증 및 전처리

#### Week 2: 모델 학습
- **Day 8-9**: BERT 파인튜닝 (3 epochs)
- **Day 10**: 성능 평가
- **Day 11**: 하이퍼파라미터 조정
- **Day 12-13**: 최종 학습 및 최적화
- **Day 14**: 데모 준비 및 문서화

## 4. 평가 지표

### 4.1 정량적 평가

- **벡터 유사도**: 동일 의미 → 코사인 유사도 > 0.9
- **변환 정확도**: 팩트 추출 정확도 80% 이상
- **일관성**: 동일 엔티티 → 일관된 벡터

### 4.2 정성적 평가

- 주요 팩트 질문 100개 테스트
- 관계 추론 50개 테스트
- 속성 추출 50개 테스트

## 5. 리스크 관리

### 기술적 리스크

| 리스크 | 대응 방안 |
|--------|-----------|
| GPT 생성 품질 저하 | 수동 검수 비율 증가 |
| 학습 시간 초과 | 데이터셋 축소 (5만개) |
| 과적합 | Early Stopping, Dropout 증가 |

### 일정 리스크

- 버퍼 시간: 각 단계별 20% 여유
- 최소 목표: 5만개 데이터셋으로도 PoC 가능

## 6. 성과물

### 최종 산출물

- **GEUL 인코더 모델** (HuggingFace 공개)
- **학습 데이터셋** (10만개, 공개)
- **변환 API** (데모 서버)
- **기술 문서** (GitHub)
- **성능 벤치마크** (논문 초안)

### 성공 기준

- ✅ 팩트 기반 질의 변환율 70% 이상
- ✅ 추론 속도 100ms 이내
- ✅ 재현 가능한 오픈소스 공개