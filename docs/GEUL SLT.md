SLT.md

1. 개요: SLT란 무엇인가

SLT(Shallow Logic Transformer) 는 글월/GEUL 기반 글DBMS 내부에 탑재되는 초경량 논리 신경망 모듈이다.

입력:

쿼리 컨텍스트 (현재 씬/사용자 요구/GEUL 쿼리 등)

리소스 (인물, 무기, 장소, 씬, 이벤트 등)


출력:

단 하나의 실수 값



\text{score} \in [0.0, 1.0]

SLT는 생성(Generate)을 전혀 하지 않고,
오직 “더 잘 찾기(Better Retrieve)”를 위한 점수기(Scorer) 역할만 수행한다.


---

2. 전체 아키텍처에서 SLT의 위치

글월/GEUL 기반 월드 엔진의 대략적인 계층 구조는 다음과 같다.

1. 심볼릭/GEUL + SIMD 쿼리 레이어 (글DBMS 코어)

의미정렬 ID + SIMD 비트마스크 + N-hop 그래프 탐색

역할:

카테고리/타입/상태/씬/동사 조건으로 하드 필터링

후보군을 수십만 → 수천/천 단위까지 좁히는 역할




2. SLT 레이어 (얕은 논리 트랜스포머)

입력: (쿼리, 리소스) 쌍

출력: [0,1] 적합도 점수

역할:

심볼릭 필터로 남은 후보들 사이에서
“누가 지금 맥락에 더 잘 맞는지” 재정렬/선별




3. 상위 LLM 레이어 (GPT 등)

입력: SLT가 선별한 상위 K개 리소스 + 세계관/씬 컨텍스트

출력:

서술, 대사, 묘사, 설명, 플롯 선택, 독자 반응 가설 등


역할:

고차원 추론 + 자연어 생성 담당





정리하면:

> 글DBMS(심볼릭): 누가 “후보 자격”이 있는가
SLT: 그 후보들 중 “누가 더 잘 맞는가” (점수만)
LLM: 그 선택을 가지고 무엇을 쓸 것인가 (생성/표현)




---

3. SLT의 역할 정의

SLT의 역할은 아래 한 줄로 규정할 수 있다.

> SLT는 f(쿼리, 리소스) → [0.0, 1.0] 적합도 점수만 낸다.
세계관 법칙, 최종 선택, 서술 생성은 SLT의 역할이 아니다.



구체적으로:

SLT 하는 일

후보 리소스들에 대해:

분위기/역할/미학/논리적 적합도 등을 반영해 스코어를 매김


쿼리 시:

ORDER BY SCORE(...) DESC LIMIT K

또는 WHERE SCORE(...) >= threshold 형태로 필터링에 참여



SLT 하지 않는 일

“이 설정은 최종적으로 맞다/틀리다”라는 확정 판정

세계관 규칙(법칙)을 위반하는 하드한 덮어쓰기

자연어 서술, 텍스트 생성



SLT는 어디까지나 **“취향/맥락/논리 적합도 필터”**이며,
최종적인 결정권은 심볼릭 제약 + 상위 LLM + 작가에게 있다.


---

4. 쿼리 파이프라인에서의 사용 방식

SLT는 SQL/쿼리 레벨에서 스칼라 함수처럼 사용된다.

예시:

SELECT id, name
FROM RESOURCE
WHERE category = 'WEAPON'
  AND subtype IN ('SWORD', 'LONG_SWORD')
ORDER BY SCORE('용사의 검으로 쓸만한 무기', id) DESC
LIMIT 10;

category, subtype 조건:

GEUL/심볼릭 쿼리 + SIMD 비트마스크로 처리되는 하드 필터


SCORE(...):

SLT가 계산하는 쿼리–리소스 적합도 점수


LIMIT 10:

상위 10개만 상위 LLM 혹은 UI에 전달



추가적으로, 다음과 같은 WHERE 조건도 가능하다.

... WHERE SCORE('용사의 검', id) >= 0.8

score >= 0.8 이상인 리소스만 후처리 대상으로 삼는 구조.


이렇게 하면:

초보 사용자: 단순히 ORDER BY SCORE(...) LIMIT 10만 써도 되고

고급 사용자: WHERE SCORE(...) >= threshold 로 최소 적합도를 직접 제어할 수 있다.



---

5. SLT 내부 구조 (초경량 스펙)

5.1 기본 아이디어

SLT는 쿼리 + 리소스 정보를 하나의 시퀀스로 인코딩한 뒤,
얕은 트랜스포머로 그 상관성을 평가하고, 마지막에 점수 하나를 뽑는다.

입력 예:

쿼리 측:

"용사의 검으로 쓸만한 무기" 를 GEUL/메타 토큰으로 표현

현재 씬 분위기/장르/Mood: [HEROIC] [EPIC] [LIGHT] 등


리소스 측:

리소스의 타입/속성/태그:

WEAPON, SWORD, HOLY, LEGENDARY, RARITY=EPIC, …


리소스 설명/백스토리 요약 GEUL 토큰



합쳐서:

[쿼리 토큰들] + [리소스 토큰들]

을 하나의 시퀀스로 SLT에 넣는다.


---

5.2 초경량 SLT 제안 스펙

실전용 기본 스펙 (예시)

최대 토큰 길이: L ≈ 256

쿼리 + 리소스 메타 합쳐서 256 토큰 안에 들어오도록 설계


임베딩 차원: d_model = 256

FFN 차원: d_ff = 1024

Transformer Encoder 레이어 수: 2

어텐션 헤드 수: 4 (head당 64차원)

GEUL vocab: 65,536 토큰 가정

출력 헤드:

h_cls → Linear(256→1) → Sigmoid 로 [0,1] 점수



5.3 모델 크기 (FP16 기준)

대략적인 파라미터 수:

임베딩: 65,536 × 256 ≈ 16.8M

Transformer 2층: ≈ 1.6M

기타(LayerNorm, 출력 헤드 등): 수만~수십만 단위


합계 ≈ 18.3M 파라미터

FP16(2바이트) 기준 인메모리 용량:

약 36–40MB 정도



즉, H100(80GB VRAM) 기준으로 볼 때:

SLT 100개 상주: ≈ 4GB

SLT 200개 상주: ≈ 8GB


로, 충분히 다수의 SLT를 VRAM에 상주시킬 수 있는 수준이다.


---

6. 성능/지연 시간 관점

6.1 후보 수와 연산량

설계 원칙:

SIMD + 의미정렬 ID + 심볼릭 쿼리로
SLT 앞단 후보를 최대한 줄인다.

목표:

일반적인 경우: 후보 ≤ 1,000건

최악의 경우: 후보 ≤ 10,000건



초경량 스펙(위의 2-layer, d=256, L≈256) 기준:

SLT 1회 추론은 sub-GFLOPs 수준

후보 1,000건 × SLT 2–3개를 돌려도:

H100 기준으로 수 ms~수십 ms 사이


후보 10,000건 × SLT 2–3개인 최악의 경우에도:

0.x 초 이내에 충분히 들어오는 범위



즉:

> **“SIMD로 후보 1,000건 이하로 쳐낸 뒤,
그 위에 SLT 2–3개를 돌린다”**는 전략은
H100급 환경에서 성능·지연 시간 측면 모두 현실적인 설계이다.




---

7. 기본 SLT vs 커스텀 SLT

글DBMS는 크게 두 종류의 SLT를 상정한다.

1. 기본 논리 SLT 세트 (Built-in)

논리학/물리량/기본 감정·역할 축 등,
“어디서나 유용한 공통 축”들

예:

Causal_Plausibility_SLT : 인과 개연성

Temporal_Coherence_SLT : 시간축 정합성

Physical_Plausibility_SLT : 물리적으로 가능한지

Heroic_SLT, Villainous_SLT, Tragic_SLT 등 서사 축

기본 감정 축: Sadness, Hope, Tension, Calmness …




2. 커스텀 SLT (사용자/프로젝트 전용)

특정 작가/프로젝트/장르를 위해 별도 학습한 SLT

예:

“내 작품 세계관에서 ‘신성함’이란 무엇인가”

“이 작가의 전작에서 ‘주인공답다’는 느낌의 패턴”


API:

사용자가 자신의 데이터로 SLT 헤드만 파인튜닝하거나,

아예 전용 SLT를 추가로 탑재할 수 있도록 설계





이렇게 하면,

기본 SLT로도 웬만한 서사/장르 작업은 커버하고,

고급 사용자/프로젝트는 고유한 논리·미학 기준을 SLT로 학습해
자신만의 “필터 렌즈”를 만들 수 있다.



---

8. SLT 라이브러리: “논리 필터 레이어”로서의 의미

SLT를 여러 개 쌓아두면, 글DBMS는 단순한 저장소를 넘어
**“논리 탐색이 가능한 지능형 DB”**가 된다.

예:

쿼리: “주인공이 탈출할 때 쓸만한 빠르고 튼튼한 탈것”

동작:

1. 심볼릭/SIMD:

Category=VEHICLE으로 후보 수천 개 생성



2. SLT 조합:

Speed_SLT (가중치 1.0)

Durability_SLT (가중치 0.8)

Heroic_SLT (가중치 0.5)



3. 종합 점수:




\text{Score} = 1.0·\text{Speed} + 0.8·\text{Durability} + 0.5·\text{Heroic}

비슷한 방식으로:

“위험하지만 아름다운 존재”
→ Danger_SLT + Beauty_SLT 조합

“조금 슬프면서 장엄한 장면 후보”
→ Sadness_SLT + Majestic_SLT 조합


이 가능해진다.

핵심:

> 리소스 벡터를 미리 고정해놓고 한 번의 코사인 유사도로 끝내는
전통적인 Vector DB가 아니라,
**“여러 SLT를 갈아끼우며 관점을 재구성하는 검색 엔진”**으로 진화한다.




---

9. SLT 학습 전략 (개요)

SLT는 작고 얕기 때문에, 아래와 같은 데이터로 충분히 학습 가능하다.

1. 작가 선택 로그

글월 UI에서:

시스템이 후보 10개 제안

작가가 실제로 선택한 1~2개


선택된 리소스: label 1에 가깝게

버려진 리소스: label 0에 가깝게

→ 랭킹/분류 문제로 SLT 학습



2. 상위 LLM Distillation

GPT 등에게:

“이 20개의 무기 중, ‘용사의 검’으로 더 어울리는 순서를 매겨라”


그 랭킹을 기반으로:

상위는 score↑, 하위는 score↓로 SLT를 증류(distill)




3. 룰북/세계관 기반 Synthetic 데이터

장르 룰북/세계관 설정에서:

“영웅 무기는 holy/unique/legendary 속성을 가진다”


이를 바탕으로 positive/negative 샘플을 합성해 SLT 학습




중요한 점:

SLT는 “전지적 생성 모델”이 아니라,
매우 제한된 역할(점수 매기기)만 하기 때문에,

데이터 요구량도 상대적으로 적고,
디버깅/튜닝/대체도 쉬운 편이다.



---

10. 정리

SLT(Shallow Logic Transformer) 는
글DBMS 내부에 탑재되는 초경량 논리 스코어링 모듈이다.

역할:

심볼릭/GEUL + SIMD 쿼리로 걸러진 후보 리소스들에 대해
쿼리/맥락과의 적합도를 [0,1]로 점수화한다.

오직 “점수 매기기”만 수행하며,
생성·최종 결정·세계관 법칙 판단은 하지 않는다.


스펙:

예시 초경량 모델 기준:

2-layer Transformer, d=256, L≈256, vocab=65k

FP16 기준 약 40MB 이하


H100 환경에서:

후보 1,000건 × SLT 2–3개 → 수 ms~수십 ms 이내 처리 가능



의미:

SLT를 수십 개의 “논리/미학 필터 헤드”로 구비하면,
글DBMS는 단순한 저장소가 아니라
**“맥락 인식형, 논리 탐색이 가능한 지능형 월드 DB”**로 동작하게 된다.

그리고 그 위에 GPT 같은 LLM이 서술/플롯/설명을 담당함으로써,
정합성 + 의미 + 스타일이 결합된 창작 워크플로우가 완성된다.
