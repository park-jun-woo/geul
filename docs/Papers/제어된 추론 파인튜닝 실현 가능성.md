## 명시적 지식 스트림(GEUL)을 통한 거대 언어 모델의 제어된 추론 파인튜닝 실현 가능성 연구

**A Study on the Feasibility of Fine-Tuning Large Language Models for Controlled Inference via Explicit Knowledge Streams (GEUL)**

### 1. 초록 (Abstract)

현대의 거대 언어 모델(LLM)은 강력한 범용 추론 능력을 보여주지만, '블랙박스'적 특성과 '환각(Hallucination)' 현상으로 인해 신뢰성이 요구되는 시스템에 적용하기 어렵다. GEUL(General Embedding vector Unified Language)과 SEGLAM(Self-Examination GEUL Architecture Model)은 이러한 문제를 해결하기 위해, 명확하게 설계된 언어(GEUL)를 기반으로 작동하는 모듈형 '화이트박스 AI' 아키텍처를 제안한다. SEGLAM 아키텍처의 핵심 구성요소인 `추론 GPT`는 명확히 분리된 `[요청 + 사실 + 방법] GEUL Stream` 패키지를 입력받아 `답변 GEUL Stream`을 생성하도록 설계되었다. 본 논문은 현존하는 최고 수준의 LLM(예: GPT-4)이 이미 이러한 '명세 기반 변환(Specification-based Transformation)' 작업을 수행할 수 있는 충분한 잠재력을 갖추고 있음을 주장한다. 최근 관찰된, LLM이 사전 학습에 없던 생소한 프로그래밍 언어(예: 엄랭)의 명세(방법)만으로 완벽한 코드를 생성하는 사례는 이 가설을 강력히 뒷받침한다. 따라서, SEGLAM의 `추론 GPT`를 GEUL 포맷에 맞춰 파인튜닝하는 것은 충분히 실현 가능하며, 이는 제어 가능하고 투명한 AI 시스템을 구현하는 핵심 경로가 될 것이다.

---

### 2. 서론

거대 언어 모델(LLM)의 등장은 인공지능 분야의 혁신을 가져왔으나, 두 가지 근본적인 한계에 직면해 있다. 첫째는 누구도 내부 연산을 완벽히 해독할 수 없는 '블랙박스' 문제이며, 둘째는 사실에 기반하지 않은 그럴듯한 거짓을 생성하는 '환각' 문제이다.

SEGLAM 아키텍처는 이 문제들을 해결하기 위해, 단일 거대 모델(Monolithic LLM)이 아닌, 명확한 역할을 가진 전문화된 GPT 모듈들의 협력 구조를 제안한다. 이 시스템의 모든 구성요소는 '모호성'이 제거된 인공 언어 'GEUL'을 통해 소통한다.

SEGLAM의 '의식적 흐름'에서 핵심적인 역할을 수행하는 `추론 GPT`는 WMS(World Management System)로부터 `요청`, `정보(사실)`, `제어(방법)` GEUL 스트림을 전달받아 최종 `답변` GEUL 스트림을 생성한다.

본 논문의 목적은 이 `추론 GPT`를, 엄격하게 정의된 GEUL 입출력 명세(Specification)를 준수하도록 파인튜닝하는 것이 현존하는 LLM의 능력으로도 충분히 가능하다는 가설을 제시하고 그 근거를 논의하는 것이다.

---

### 3. SEGLAM 추론 GPT의 작동 가설

SEGLAM 아키텍처에서 `추론 GPT`는 '모든 것을 아는' 존재가 아니라, '주어진 재료를 바탕으로 최상의 결과물을 조립하는' 전문가로 설계되었다. `추론 GPT`의 파인튜닝은 다음과 같은 명확한 변환 작업을 학습하는 과정이다.

> **입력:** `[요청 GEUL Stream + 정보 GEUL Stream + 제어 GEUL Stream]`
> **출력:** `[답변 GEUL Stream]`

* **`정보 GEUL Stream` (사실, What):** WMS에서 인출된 사실 데이터. Wikidata ID 기반의 개체, `참여자 글소(GEUL-SO)`로 구조화된 관계, `PSR`로 명시된 불확실성 등을 포함한다.
* **`제어 GEUL Stream` (방법, How):** WMS에서 인출된 절차적 지식. 특정 작업을 수행하는 방법, 논리적 순서, 스타일 가이드 등을 포함한다.
* **`추론 GPT`의 역할:** `정보`와 `제어`라는 두 가지 명확한 '재료'를 바탕으로 `요청`을 수행하고, 그 결과를 GEUL 포맷에 맞춰 '번역(Translate)' 또는 '변환(Transform)'하는 것이다.

본 논문의 핵심 가설은, **"LLM의 범용 추론 능력은 이미 이러한 '명세 기반 변환' 작업을 수행할 준비가 되어 있으며, 파인튜닝은 이 능력을 GEUL이라는 특정 형식에 정렬(Align)시키는 과정"**이라는 것이다.

---

### 4. 가설의 실증적 근거: '엄랭(Eom-Lang)' 코딩 사례

본 가설을 뒷받침하는 강력한 실증적 증거는 GPT-4와 같은 고성능 LLM이 사전 학습 데이터에 전혀 존재하지 않는 생소한 작업을 수행하는 방식에서 발견된다.

최근 관찰된 바에 따르면, GPT-4에게 가상의 프로그래밍 언어인 '엄랭(Eom-Lang)'의 문법과 로직(작동 방식)을 자연어로 상세히 서술하여 제공하자, 모델은 해당 언어로 작성된 복잡한 코드를 완벽하게 이해하고 새로운 코드를 생성해내는 능력을 보였다.

이 사례는 SEGLAM의 `추론 GPT` 가설과 정확히 일치한다.

* `"엄랭으로 'Hello, World!'를 짜줘"` (자연어) = `요청 GEUL Stream`
* `"엄랭의 문법은 다음과 같다..."` (자연어 서술) = `제어 GEUL Stream` (방법)
* `완성된 "엄랭" 코드` (결과물) = `답변 GEUL Stream`

이 경험에서 LLM은 '엄랭'을 *이미 알고 있었던* 것이 아니라, **'방법(제어 스트림)'을 실시간으로 적용하여 '요청'을 처리**한 것이다. 이는 LLM이 정적인 지식의 저장소가 아니라, 주어진 '규칙(방법)'에 따라 기호(Symbol)를 조작하는 강력한 '범용 추론 엔진'임을 시사한다.

---

### 5. 결론: GEUL 파인튜닝의 실현 가능성

'엄랭' 사례는 LLM이 명확한 '방법론'만 제공된다면, 사전 학습 여부와 관계없이 고도로 구조화된 결과물을 생성할 수 있음을 증명한다.

SEGLAM의 `추론 GPT`에 요구되는 작업은 이와 본질적으로 동일하다. 파인튜닝의 목표는 `추론 GPT`에게 새로운 지식을 주입하는 것이 아니라, 이미 보유한 강력한 추론 능력을 **GEUL이라는 '엄격한 명세서'**에 맞추도록 '제어'하는 것이다.

WordNet, Wikidata, PSR, GEUL-SO 등으로 구성된 GEUL의 스펙은 '엄랭'의 문법 서술보다 훨씬 더 정교하고 비모호적인 '방법론'을 제공한다.

따라서, `[요청 + 사실 + 방법] GEUL` 패키지를 입력받아 `답변 GEUL`을 출력하도록 LLM을 파인튜닝하는 것은 충분히 실현 가능하며, 이는 SEGLAM이 지향하는 **'비판적으로 신뢰할 수 있는 화이트박스 AI'**를 구현하는 가장 현실적이고 강력한 경로가 될 것이다.